<!DOCTYPE html>
<html lang="en"><head>
<script src="2023_12_07_MLJC_files/libs/clipboard/clipboard.min.js"></script>
<script src="2023_12_07_MLJC_files/libs/quarto-html/tabby.min.js"></script>
<script src="2023_12_07_MLJC_files/libs/quarto-html/popper.min.js"></script>
<script src="2023_12_07_MLJC_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="2023_12_07_MLJC_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2023_12_07_MLJC_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="2023_12_07_MLJC_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="2023_12_07_MLJC_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.433">

  <title>Graph Representation Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="2023_12_07_MLJC_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="2023_12_07_MLJC_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="2023_12_07_MLJC_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="styles.css">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="2023_12_07_MLJC_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Graph Representation Learning</h1>
  <p class="subtitle">From Graph ML to Temporal GNNs</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<a href="https://github.com/ClaudMor">Claudio Moroni</a> <a href="https://orcid.org/0000-0003-1274-6937" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>
        <p class="quarto-title-affiliation">
            University of Turin
          </p>
        <p class="quarto-title-affiliation">
            CENTAI Institute S.p.A.
          </p>
    </div>
</div>

</section>
<section class="slide level2">

<!-- https://github.com/quarto-dev/quarto-cli/discussions/4572 and  https://github.com/quarto-dev/quarto-cli/discussions/2951-->
<style>
.semi-transparent {
  opacity: 0.3;
}
</style> 



<style>
.center-x {
    margin: 0;
    position: absolute;
    top: 30%;
    left: 50%;
    -ms-transform: translateY(-0%), translateX(-50%);
    transform: translateY(-0%), translateX(-50%);
}
</style>

<style>
.justify {
  text-align: center !important
}
</style>

<!-- ```{=css, echo=FALSE}
.justify {
  text-align: center !important
}
``` -->
<p><br></p>
<h3 id="outline">Outline</h3>
<p><span class="emoji" data-emoji="one">1Ô∏è‚É£</span> Introduction to Deep Learning</p>
<p><span class="emoji" data-emoji="two">2Ô∏è‚É£</span> Graph Representation Learning</p>
</section>
<section>
<section id="introduction-to-deep-learning" class="title-slide slide level1 center">
<h1>Introduction to Deep Learning</h1>
<ol type="1">
<li>Introduction to Deep Learning</li>
</ol>
</section>
<section id="introduction-to-deep-learning-1" class="slide level2 smaller">
<h2><span class="semi-transparent">Introduction to Deep Learning</span></h2>
<ol type="1">
<li><p>Introduction to Deep Learning:</p>
<p>1.1 Problem Setting</p>
<p>1.2 The ‚ÄúDeep Learning‚Äù Solution</p>
<p>1.3 Gradient Descent</p>
<p>1.4 Deep Learning on Graphs</p></li>
</ol>
</section>
<section id="problem-setting-1" class="slide level2 smaller">
<h2>Problem Setting #1</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<ul>
<li>Usual problem in <strong>Mathematics/Physics</strong>: given a ‚Äúsetting‚Äù, compute a <strong>quantity</strong>/<strong>functional form</strong>;</li>
</ul>
<div class="fragment">
<ul>
<li><strong>Ex1</strong>: Given hydrogen atom, compute energy levels;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>Ex2</strong>: Given two points on a curved surface, find the equation for the shortest path;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>In all cases, a more or less <strong>intuitive</strong> but <strong>explicit</strong> sequence of operations allows for a ‚Äú<strong>proof</strong>‚Äù;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>But what if a problem does <strong>not</strong> admit a proof? E.g. :</p>
<ul>
<li><p>The intuition required <strong>surpasses</strong> human capability;</p></li>
<li><p>The problem is not completely <strong>specified</strong>.</p></li>
</ul></li>
</ul>
</div>
</section>
<section id="problem-setting-2" class="slide level2 smaller">
<h2>Problem Setting #2</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<p><strong>EXAMPLE</strong>: Dog vs Cat classification</p>

<img data-src="images/cat_vs_dog.jpg" class="r-stretch quarto-figure-center"><div class="fragment">
<p>Other Tasks: Text Classification, Image Segmentation, Speech to Text, Scientific Machine Learning, Law Discovery, Anti Money Laundering‚Ä¶</p>
</div>
</section>
<section id="problem-setting-3" class="slide level2 smaller">
<h2>Problem Setting #3</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<p>What we <strong>have</strong>:</p>
<ul>
<li>A set of images (<span class="math inline">\(n \times n\)</span> arrays), whose labels we <strong>know</strong>: <span class="math display">\[\mathcal{D} = \left\{ (X_1, "dog"), (X_2, "cat"), ..., (X_n, "dog") \right\}\]</span></li>
</ul>
<div class="fragment">
<ul>
<li>A set of images (<span class="math inline">\(n \times n\)</span> arrays), whose label we‚Äôd like to correctly <strong>predict</strong>: <span class="math display">\[\mathcal{D'} = \left\{ X_{1'}, X_{2'}, ..., X_{n'} \right\}\]</span></li>
</ul>
</div>
<div class="fragment">
<p>What we <strong>want</strong>:</p>
<ul>
<li>A <strong>function</strong> <span class="math inline">\(f^*\)</span> that maps images to their <strong>correct</strong> label, both in <strong>train</strong> and <strong>test</strong> sets.</li>
</ul>
</div>
<div class="fragment">
<p>Similarly for the other tasks.</p>
</div>
<div class="fragment">
<p>These problems are clearly <strong>very complicated</strong> and <strong>underspecified</strong>.</p>
</div>
</section>
<section id="the-deep-learning-solution" class="slide level2 smaller">
<h2>The ‚ÄúDeep Learning‚Äù Solution</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<p>One fits them all:</p>
<div class="fragment">
<div class="absolute" style="top: 200px; ">
<div class="justify">
<p>Write a <strong>function family</strong> <span class="math inline">\(f_{\vec{\theta}}\)</span> ‚Äúflexible enough‚Äù to include the target function <span class="math inline">\(f^*\)</span>, then find <span class="math inline">\(\vec{\theta}^*\)</span> s.t. <span class="math inline">\(f_{\vec{\theta}^*} \approx f^*\)</span></p>
</div>
</div>
<div class="absolute" style="top: 400px; ">
<ul>
<li class="fragment"><p>In our case, we‚Äôd look for a <span class="math inline">\(f_{\vec{\theta}}(X_i) \in \{"dog", "cat"\}\)</span> such that mappings are correct;</p></li>
<li class="fragment"><p>Very expressive functions: <strong>Neural Networks</strong> <span class="math inline">\(f_{\vec{\theta}} = \sigma (W_0 \sigma(W_1 \sigma(...(\sigma(W_n X)))))\)</span>;</p></li>
<li class="fragment"><p>Parameters <span class="math inline">\(\vec{\theta} = (W_0, ..., W_n)\)</span> iteratively updated via <strong>Gradient Descent</strong>.</p></li>
</ul>
</div>
</div>
</section>
<section id="gradient-descent" class="slide level2 smaller">
<h2>Gradient Descent</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<ul>
<li>Choose <strong>differentiable point-wise ‚Äúloss‚Äù</strong> e.g.&nbsp;(‚Äúdog‚Äù/‚Äúcat‚Äù <span class="math inline">\(\iff y_i = 1/0\)</span>):</li>
</ul>
<p><span class="math display">\[\mathcal{l}(y_i, f_{\vec{\theta}}(X_i)) =  P(y_i | X_i ; f_{\vec{\theta}}(X_i) ) = f_{\vec{\theta}}(X_i)^{y_i} (1-f_{\vec{\theta}}(X_i))^{1-y_i}\]</span></p>
<div class="fragment">
<ul>
<li>Define <strong>dataset-wise loss</strong>:</li>
</ul>
<p><span class="math display">\[\mathcal{L}(D, f_{\vec{\theta}}) = - ln \left( \prod\limits_{i = 1}^n \mathcal{l}(y_i, f_{\vec{\theta}}(X_i)) \right)\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>Iteratively update</strong> parameters:</li>
</ul>
<p><span class="math display">\[\vec{\theta} \leftarrow \vec{\theta} - \nabla_{\vec{\theta}} \mathcal{L}(D, f_{\vec{\theta}})\]</span></p>
</div>
<div class="fragment">
<p>Until <strong>convergence</strong>.</p>
</div>
</section>
<section id="deep-learning-on-graphs" class="slide level2 smaller">
<h2>Deep Learning on Graphs</h2>
<div class="footer">
<p>Introduction to Deep Learning</p>
</div>
<p>Given a Graph:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/network.png" height="150"></p>
</figure>
</div>
<div class="fragment">
<p>Possible Tasks:</p>
<ul>
<li><p>Node Classification;</p></li>
<li><p>Link Prediction/Classification;</p></li>
<li><p>Graph Classification;</p></li>
<li><p>Subgraph Matching;</p></li>
<li><p>‚Ä¶</p></li>
</ul>
<p><strong>Q</strong>: What ‚Äúfunctional family‚Äù <span class="math inline">\(f_{\vec{\theta}}\)</span> do we use for Deep Learning on (Temporal) Graphs?</p>
</div>
</section></section>
<section>
<section id="graph-representation-learning" class="title-slide slide level1 center">
<h1>Graph Representation Learning</h1>
<ol type="1">
<li><p>Graph Machine Learning</p></li>
<li><p>Graph Deep Learning</p></li>
<li><p>Temporal Graph Deep Learning</p></li>
</ol>
</section>
<section id="graph-representation-learning-1" class="slide level2 smaller">
<h2><span class="semi-transparent">Graph Representation Learning</span></h2>
<ol type="1">
<li><p>Graph Machine Learning:</p>
<p>1.1 Early Models</p>
<p>1.2 Weisfeiler-Leman Kernel</p></li>
</ol>
<p><span class="semi-transparent">2. Graph Deep Learning</span></p>
<p><span class="semi-transparent">2. Temporal Graph Neural Networks</span></p>
</section>
<section id="early-models" class="slide level2 smaller">
<h2>Early Models</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Machine Learning</p>
</div>
<ul>
<li>Before Deep Learning, features were <strong>human-engineered</strong> (Machine Learning);</li>
</ul>
<div class="fragment">
<ul>
<li>Early works can be organized in node/edge/graph-level <strong>features</strong>;</li>
</ul>
</div>
<div class="fragment">
<p><strong>Node-level</strong>:</p>
<ul>
<li><p><strong>Features</strong>: Degree, Eigenvector, Betweeness, Closeness <strong>centralities</strong> &amp; Clustering Coefficient (<strong>CC</strong>)/Graphlet Degree Vector (<strong>GDV</strong>)</p></li>
<li><p><strong>RW-based</strong>: PageRank, <strong>DeepWalk</strong>, <strong>Node2Vec</strong> (not inductive!);</p></li>
</ul>
</div>
<div class="fragment">
<p><strong>Edge-level</strong>: Local/Gobal Neighborhood overlap (<strong>Katz Index</strong>)</p>
</div>
<div class="fragment">
<p><strong>Graph-level</strong>: (Causal) Anonymous Walks, Graphlet Kernel, <strong>Weisfeiler-Leman Kernel</strong>.</p>
</div>
</section>
<section id="weisfeiler-leman-kernel-1" class="slide level2">
<h2>Weisfeiler-Leman Kernel #1</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Machine Learning</p>
</div>
<p><u><strong>Def.</strong>(Graph Isomorphism)</u> : Two graphs <span class="math inline">\((V_1, E_1)\)</span> and <span class="math inline">\((V_2, E_2)\)</span> are <strong>isomorphic</strong> <span class="math inline">\(\iff\)</span> <span class="math display">\[\exists \text{ bijection }f: V_1 \rightarrow V_2 | (u,v) \in E_1 \implies (f(u), f(v)) \in E_2\]</span></p>
<div class="fragment">
<p><u><strong>Obs.</strong></u> : Determining <span class="math inline">\(f\)</span> is extremely <strong>expensive</strong> <span class="math inline">\(\implies\)</span> need for <strong>approximate</strong> algorithms <span class="math inline">\(\implies\)</span> <strong>WL-kernel</strong></p>
</div>
</section>
<section id="weisfeiler-leman-kernel-2" class="slide level2 smaller scrollable">
<h2>Weisfeiler-Leman Kernel #2</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Machine Learning</p>
</div>
<p><strong>WL-Kernel</strong>:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ol type="1">
<li><p>Assign initial color <span class="math inline">\(c^{(0)}_v\)</span> to each node <span class="math inline">\(v\)</span>;</p></li>
<li><p>Iteratively <strong>refine</strong> colors via: <span class="math display">\[c^{(k+1)}_v = HASH\left(c^{(k)}_v, \{ c^{(k)_u | u \in N(v)} \} \right)\]</span></p></li>
<li><p>Terminate when colors <strong>stabilize</strong> (#nodes iterations maximum);</p></li>
<li><p>Assign to each graph the empirical color distribution (histogram)</p></li>
</ol>
</div><div class="column scrollable" style="width:50%;">
<ol type="1">
<li><p><img data-src="images/WL_1.png" data-fig-align="center" height="150"> <!-- fig-align="center" height=200 --></p></li>
<li><p><img data-src="images/WL_2.png" data-fig-align="right" height="150"> <!-- height=200 --></p></li>
<li><p><img data-src="images/WL_3.png" data-fig-align="right" height="150"> <!-- height=200 --></p></li>
<li><p><img data-src="images/WL_4.png" data-fig-align="right" height="150"> <!-- height=200 --></p></li>
</ol>
</div>
</div>
</section>
<section id="weisfeiler-leman-kernel-3" class="slide level2 smaller">
<h2>Weisfeiler-Leman Kernel #3</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Machine Learning</p>
</div>
<p><u><strong>Obs.</strong></u> WL-Kernel <strong>cannot distinguish</strong> all non-isomorphic graphs e.g.:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/WL_graphs.png" height="150"></p>
</figure>
</div>
<div class="fragment">
<p>And computes <strong>same color</strong> for different nodes:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/WL_nodes.png" height="150"></p>
</figure>
</div>
</div>
<div class="fragment">
<p><u><strong>Obs.</strong></u> : GNNs will be <strong>differentiable</strong> versions of <span class="math inline">\(HASH\)</span> <span class="math inline">\(\implies\)</span> less expressive.</p>
</div>
</section>
<section id="graph-representation-learning-2" class="slide level2 smaller">
<h2><span class="semi-transparent">Graph Representation Learning</span></h2>
<p><span class="semi-transparent">Graph Machine Learning</span></p>
<ol start="2" type="1">
<li><p>Graph Deep Learning</p>
<p>2.1 Historical Derivation of GNNs</p>
<p>2.2 Message Passing Framework</p>
<p>2.3 Expressivity of GNNs</p></li>
</ol>
<p><span class="semi-transparent">3. Temporal Graph Deep Learning</span></p>
</section>
<section id="historical-derivation-of-gnns-1" class="slide level2">
<h2>Historical Derivation of GNNs #1</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<p><strong>Disadvantages</strong> of previous methods:</p>
<ul>
<li>Inherently <strong>transductive</strong>;</li>
</ul>
<div class="fragment">
<ul>
<li>Do not <strong>incorporate</strong> node/edge/graph features;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Computed node/edge/graph features are <strong>task-independent</strong>;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Not <strong>End-to-End</strong>.</li>
</ul>
</div>
<div class="fragment">
<div class="absolute" style="top: 600px; ">
<p><span class="math inline">\(\implies\)</span> Need for Graph Deep Learning i.e.&nbsp;<strong>GNNs</strong>!</p>
</div>
</div>
</section>
<section id="historical-derivation-of-gnns-2" class="slide level2 smaller scrollable">
<h2>Historical Derivation of GNNs #2</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<p><strong>Historical</strong> derivation of GNNs:</p>
<ol type="1">
<li>Simultaneous discretization of a Riemannian Manifold <span class="math inline">\(\mathcal{M}\)</span> &amp; Laplace-Beltrami operator yields a graph <span class="math inline">\(G\)</span> and its laplacian <span class="math inline">\(L = D - A\)</span><sup>1</sup></li>
</ol>
<div class="fragment">
<ol start="2" type="1">
<li><span class="math inline">\(\hat{L} = D^{-\frac{1}{2}} L D^{-\frac{1}{2}} = I - \hat{A}\)</span>‚Äôs eigenvectors <span class="math inline">\(U\)</span> used to define the Graph Fourier Transform: <span class="math display">\[\mathcal{F}(\vec{x}) = U^{T}\vec{x}\]</span></li>
</ol>
</div>
<div class="fragment">
<ol start="3" type="1">
<li><span class="math inline">\(\mathcal{F}\)</span> used to define Graph Convolution:</li>
</ol>
<p><span class="math display">\[\vec{g}_{\vec{\theta}} * \vec{x} = U \vec{g}_{\vec{\theta}} U^T \vec{x}\]</span></p>
</div>
<div class="fragment">
<ol start="4" type="1">
<li>Interpret <span class="math inline">\(\vec{g}_{\vec{\theta}}\)</span> as function of <span class="math inline">\(\hat{L}\)</span>‚Äôs eigenvalues <span class="math inline">\(\hat{\Lambda}\)</span>, and approximate using Chebishev‚Äôs polynomials: <span class="math display">\[\vec{g}_{\vec{\theta}} \approx \sum\limits_{k = 0}^{K}\theta_k T_k(\hat{\Lambda})\]</span></li>
</ol>
</div>
<div class="fragment">
<ol start="5" type="1">
<li>Truncate to <strong>first-order</strong>, <strong>group</strong> parameters, <strong>renormalize</strong>, apply <strong>non-linearity</strong> and generalize to matrices: <span class="math display">\[Z \approx \text{ReLU}\left(\tilde{A}X\Theta \right)\]</span> where <span class="math inline">\(\tilde{A} = D^{-\frac{1}{2}} (A + I) D^{-\frac{1}{2}}\)</span><sup>2</sup></li>
</ol>
</div>
<div class="fragment">
<p><u><strong>Obs.</strong></u> Alternative approach: <strong>MPNN</strong>s<sup>3</sup></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p><span class="citation" data-cites="Nahmias2017">Nahmias (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>, <span class="citation" data-cites="Hein2005">Hein, Audibert, and Luxburg (<a href="#/references" role="doc-biblioref" onclick="">2005</a>)</span>, <span class="citation" data-cites="Burago2014">Burago, Ivanov, and Kurylev (<a href="#/references" role="doc-biblioref" onclick="">2014</a>)</span>, <span class="citation" data-cites="Lu2020">Lu (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>, <span class="citation" data-cites="Aubry2013">Aubry (<a href="#/references" role="doc-biblioref" onclick="">2013</a>)</span>, <span class="citation" data-cites="GarcaTrillos2019">Garc√≠a Trillos et al. (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span></p></li><li id="fn2"><p><span class="citation" data-cites="kipf2017semisupervised">Kipf and Welling (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span></p></li><li id="fn3"><p><span class="citation" data-cites="Gilmer2017_MPNN">Gilmer et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span></p></li></ol></aside></section>
<section id="message-passing-framework-1" class="slide level2 smaller">
<h2>Message Passing Framework #1</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<p><strong>INTUITION</strong>: We‚Äôd like to produce <strong>useful representations</strong> for each node also taking into account their neighbors (ref. CNNs).</p>
<div class="columns absolute" style="top: 300px; ">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="images/MPNN_1_div.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-right">
<figure>
<p><img data-src="images/MPNN_2.png"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="message-passing-framework-2" class="slide level2 smaller">
<h2>Message Passing Framework #2</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<p>MPNN framework prescribes that a GNN is a <strong>stacking of layers</strong> of the form:</p>
<p><span class="math display">\[\vec{h}^{(k+1)}_v = \vec{\text{COMB}}_k\left(\vec{h}^{(k)}_v, \vec{\text{AGGR}}_k\left( \left\{ \vec{\text{MSG}}_k\left(\vec{h}^{(k)}_v, \vec{h}^{(k)}_u, e_{uv} \right) | u \in N(v) \right\} \right)\right)\]</span></p>

<img data-src="images/MPNN_3.png" class="r-stretch quarto-figure-center"></section>
<section id="message-passing-framework-3" class="slide level2 smaller">
<h2>Message Passing Framework #3</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<p><strong>Kipf</strong>‚Äôs convolution recovered by:</p>
<span class="math display">\[\begin{cases}
\vec{\text{COMB}}_t &amp;= \text{ReLU}(\cdot) \text{  (no self-information)} \\
\vec{\text{AGGR}}_t &amp;= \sum \\
\vec{\text{MSG}}_t\left(\vec{h}^{(t)}_v, \vec{h}^{(t)}_u, e_{uv} \right) &amp;= \frac{W_t}{\sqrt{deg(u)}\sqrt{deg(v)}}\vec{h}^{(t)}_u
\end{cases}\]</span>
<div class="fragment">
<p><u><strong>Obs.</strong></u> All architectures within this framework are <strong>permutation-invariant</strong>;</p>
</div>
<div class="fragment">
<p>Other architectures are <strong>GAT</strong>, <strong>GraphSAGE</strong>, <strong>SGCN</strong>, etc<sup>1</sup></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn4"><p><span class="citation" data-cites="Hamilton2017_GraphSAGE">Hamilton, Ying, and Leskovec (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>, <span class="citation" data-cites="veliƒçkoviƒá2018graph">Veliƒçkoviƒá et al. (<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span>,<span class="citation" data-cites="chanpuriya2022simplified">Chanpuriya and Musco (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span></p></li></ol></aside></section>
<section id="gnn-expressivity-1" class="slide level2">
<h2>GNN Expressivity #1</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<ul>
<li>We may <strong>choose</strong> to quantify GNN ‚Äúexpressivity‚Äù by the number of (sub)graphs it may distinguish when <strong>no features</strong> are present;</li>
</ul>
<div class="fragment">
<ul>
<li>In this case, GNNs are at most expressive as the WL-kernel <strong>by construction</strong>;</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>GNNs may match WL-kernel <span class="math inline">\(\iff \vec{\text{COMB}}_t \text{, } \vec{\text{AGGR}}_t\text{, } \vec{\text{MSG}}_t\)</span> are all <strong>injective</strong> on their domains;</li>
</ul>
</div>
</section>
<section id="gnn-expressivity-2" class="slide level2 smaller scrollable">
<h2>GNN Expressivity #2</h2>
<div class="footer">
<p>Graph Representation Learning | Graph Deep Learning</p>
</div>
<ul>
<li>By <strong>Kolmogorov-Arnold‚Äôs theorem</strong><sup>1</sup>, every symmetric function can be represented as:</li>
</ul>
<p><span class="math display">\[f(x_1,...,x_n) = \rho \left(\sum\limits_{i = 1}^{n} \phi(x_i) \right) \text{where $\rho \text{ and } \phi$ are injective nonlinearities}\]</span></p>
<div class="fragment">
<ul>
<li>Thus, by the <strong>Universal Approximation Theorem</strong>, the following architecture<sup>2</sup> has a chance to match WL expressivity:</li>
</ul>
<p><span class="math display">\[\vec{h}^{(k+1)}_v = \text{MLP}\left( (1+\epsilon) \vec{h}^{(k)}_v + \sum\limits_{u \in N(v)}\vec{h}^{(k)}_u \right)\]</span></p>
</div>
<div class="fragment">
<ul>
<li>More expressive GNNs may be obtained by mimicking k-WL kernel, or by extending the MPNN framework<sup>3</sup>.</li>
</ul>
</div>
<aside><ol class="aside-footnotes"><li id="fn5"><p><span class="citation" data-cites="Wagstaff2022_UAF_Sets">Wagstaff et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>, <span class="citation" data-cites="Zaheer2020_DeepSets">Zaheer et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span>, <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">wikipedia</a></p></li><li id="fn6"><p><span class="citation" data-cites="xu2018how">Xu et al. (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span></p></li><li id="fn7"><p><span class="citation" data-cites="morris2021weisfeiler">Morris et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span>, <span class="citation" data-cites="du2018topology">Du et al. (<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span>, <span class="citation" data-cites="michel2023path">Michel et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p></li></ol></aside></section>
<section id="graph-representation-learning-3" class="slide level2 smaller">
<h2><span class="semi-transparent">Graph Representation Learning</span></h2>
<p><span class="semi-transparent">1. Graph Representation Learning</span></p>
<p><span class="semi-transparent">2. Graph Deep Learning</span></p>
<ol start="3" type="1">
<li><p>Temporal Graph Deep Learning:</p>
<p>2.1 Framework</p>
<p>2.2 ROLAND</p>
<p>2.3 TGAT</p></li>
</ol>
</section>
<section id="framework" class="slide level2">
<h2>Framework</h2>
<div class="footer">
<p>Graph Representation Learning | Temporal Graph Deep Learning</p>
</div>
<p><strong>Reviews</strong>: <span class="citation" data-cites="Longa2023">Longa et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>, <span class="citation" data-cites="Gravina2023">Gravina and Bacciu (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>, <span class="citation" data-cites="Gupta2022">Gupta and Bedathur (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span></p>
<div class="fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/longa2023_taxonomy.png"></p>
</figure>
</div>
</div>
</section>
<section id="roland" class="slide level2 smaller">
<h2>ROLAND</h2>
<div class="footer">
<p>Graph Representation Learning | Temporal Graph Deep Learning</p>
</div>
<ul>
<li>A meta-model <span class="math inline">\(\text{GNN}^{(meta)}\)</span>, responsible for parameter initialization before <strong>fine tuning</strong> on each snapshot, is trained via <strong>Meta-Learning</strong> up to snapshot <span class="math inline">\(t\)</span>;</li>
</ul>
<div class="fragment">
<ul>
<li>The <span class="math inline">\(\text{GNN}_{t+1}\)</span> at snapshot <span class="math inline">\(t+1\)</span> is initialized <span class="math inline">\(GNN \leftarrow \text{GNN}^{(meta)}\)</span> and <strong>fine-tuned</strong> on snapshot <span class="math inline">\(t+1\)</span> via:</li>
</ul>
<p><span class="math display">\[H^{(t+1)}_{(k+1)} = GRU(H^{(t)}_{(k+1)}, \tilde{H}^{(t+1)}_{(k)})\]</span></p>
<p>So only <strong>current snaphsot</strong> and <strong>previous final embeddings</strong> are needed on GPU (<strong>live-update</strong>)</p>
</div>
<div class="fragment">
<ul>
<li>The meta-model is updated:</li>
</ul>
<p><span class="math display">\[GNN^{(meta)} \leftarrow (1-\alpha)GNN^{(meta)} + \alpha \text{GNN}_{t+1}\]</span></p>
</div>
<div class="fragment">
<p>And the cycle repeats.</p>
</div>
</section>
<section id="tgat" class="slide level2 smaller scrollable">
<h2>TGAT</h2>
<div class="footer">
<p>Graph Representation Learning | Temporal Graph Deep Learning</p>
</div>
<p><strong>Time differences</strong> are represented via <strong>Bochner</strong>‚Äôs theorem (translational invariance can be proved):</p>
<p><span class="math display">\[\Phi(\Delta t) = \sqrt{\frac{1}{d}} [cos(\omega_1 \Delta t), sin(\omega_1 \Delta t),...,cos(\omega_d t), sin(\omega_d \Delta t)]\]</span></p>
<div class="fragment">
<p>Given node <span class="math inline">\(v\)</span>, we define:</p>
<p><span class="math display">\[
Z^{(l-1)}(t) := [ \vec{h}_v^{(l-1)}(t) || \vec{x}_{(v,v)}|| \Phi(0), \vec{h}_{u_1}^{(l-1)}(t_1) || \vec{x}_{(u_1,v)} ||  \Phi(t-t_1),... \\ ,\vec{h}_{u_1}^{(l-1)}(t_N) || \vec{x}_{(u_N,v)} || \Phi(t-t_N)]^t \\
\vec{q}^{(l-1)}_v(t)^t := \vec{Z}^{(l-1)}_v(t) W_Q := Z^{(l-1)}(t)[0,:]W_Q \\
K^{(l-1)}(t) := Z^{(l-1)}(t)[1:N,:]W_K \\
V^{(l-1)}(t) := Z^{(l-1)}(t)[1:N,:]W_V \\
\]</span></p>
<ul>
<li>A <strong>convolutional</strong> <strong>self-attention</strong><sup>1</sup> layer is defined:</li>
</ul>
<p><span class="math display">\[\tilde{\vec{h}}_v^{(l)}(t) = \sum\limits_{u \in N(v;t)}  \underbrace{softmax \left(\frac{\vec{q}^{(l-1)}_v(t)^t \vec{K}^{(l-1)}_u(t)}{\sqrt{d}}; N(v;t)\right) \vec{V}_u^{(l-1)}(t)}_{\tilde{\vec{h}}_{uv}^{(l)}(t) }\]</span></p>
<ul>
<li>Final embedding for <span class="math inline">\(v\)</span> is:</li>
</ul>
<p><span class="math display">\[\vec{h}_v^{(l)}(t) = \text{ReLU}([\tilde{\vec{h}}_v^{(l)}(t) || \vec{h}_v^{(0)}(t)] W_0^{(l)} + \vec{b}_0^{(l)})W_1^{(l)} + \vec{b}_1^{(l)}\]</span></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn8"><p><span class="citation" data-cites="Vaswani2017_Transformer">Vaswani et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span></p></li></ol></aside></section></section>
<section id="thanks" class="title-slide slide level1 center">
<h1>Thanks <span class="emoji" data-emoji="pray">üôè</span></h1>

</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>

<img src="images/logo/centai_logo.jpeg" class="slide-logo r-stretch"><div class="footer footer-default">

</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Aubry2013" class="csl-entry" role="listitem">
Aubry, Erwann. 2013. <span>‚ÄúApproximation of the Spectrum of a Manifold by Discretization.‚Äù</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1301.3663">https://doi.org/10.48550/ARXIV.1301.3663</a>.
</div>
<div id="ref-Burago2014" class="csl-entry" role="listitem">
Burago, Dmitri, Sergei Ivanov, and Yaroslav Kurylev. 2014. <span>‚ÄúA Graph Discretization of the Laplace‚ÄìBeltrami Operator.‚Äù</span> <em>Journal of Spectral Theory</em> 4 (4): 675‚Äì714. <a href="https://doi.org/10.4171/jst/83">https://doi.org/10.4171/jst/83</a>.
</div>
<div id="ref-chanpuriya2022simplified" class="csl-entry" role="listitem">
Chanpuriya, Sudhanshu, and Cameron N Musco. 2022. <span>‚ÄúSimplified Graph Convolution with Heterophily.‚Äù</span> In <em>Advances in Neural Information Processing Systems</em>, edited by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. <a href="https://openreview.net/forum?id=jRrpiqxtrWm">https://openreview.net/forum?id=jRrpiqxtrWm</a>.
</div>
<div id="ref-du2018topology" class="csl-entry" role="listitem">
Du, Jian, Shanghang Zhang, Guanhang Wu, Jose M. F. Moura, and Soummya Kar. 2018. <span>‚ÄúTopology Adaptive Graph Convolutional Networks.‚Äù</span> <a href="https://arxiv.org/abs/1710.10370">https://arxiv.org/abs/1710.10370</a>.
</div>
<div id="ref-GarcaTrillos2019" class="csl-entry" role="listitem">
Garc√≠a Trillos, Nicol√°s, Moritz Gerlach, Matthias Hein, and Dejan Slepƒçev. 2019. <span>‚ÄúError Estimates for Spectral Convergence of the Graph Laplacian on Random Geometric Graphs Toward the Laplace‚ÄìBeltrami Operator.‚Äù</span> <em>Foundations of Computational Mathematics</em> 20 (4): 827‚Äì87. <a href="https://doi.org/10.1007/s10208-019-09436-w">https://doi.org/10.1007/s10208-019-09436-w</a>.
</div>
<div id="ref-Gilmer2017_MPNN" class="csl-entry" role="listitem">
Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 2017. <span>‚ÄúNeural Message Passing for Quantum Chemistry.‚Äù</span> In <em>Proceedings of the 34th International Conference on Machine Learning - Volume 70</em>, 1263‚Äì72. ICML‚Äô17. Sydney, NSW, Australia: JMLR.org.
</div>
<div id="ref-Gravina2023" class="csl-entry" role="listitem">
Gravina, Alessio, and Davide Bacciu. 2023. <span>‚ÄúDeep Learning for Dynamic Graphs: Models and Benchmarks.‚Äù</span> <a href="https://arxiv.org/abs/2307.06104">https://arxiv.org/abs/2307.06104</a>.
</div>
<div id="ref-Gupta2022" class="csl-entry" role="listitem">
Gupta, Shubham, and Srikanta Bedathur. 2022. <span>‚ÄúA Survey on Temporal Graph Representation Learning and Generative Modeling.‚Äù</span> <a href="https://arxiv.org/abs/2208.12126">https://arxiv.org/abs/2208.12126</a>.
</div>
<div id="ref-Hamilton2017_GraphSAGE" class="csl-entry" role="listitem">
Hamilton, William L., Rex Ying, and Jure Leskovec. 2017. <span>‚ÄúInductive Representation Learning on Large Graphs.‚Äù</span> In, 1025‚Äì35. NIPS‚Äô17. Red Hook, NY, USA: Curran Associates Inc.
</div>
<div id="ref-Hein2005" class="csl-entry" role="listitem">
Hein, Matthias, Jean-Yves Audibert, and Ulrike von Luxburg. 2005. <span>‚ÄúFrom Graphs to Manifolds ‚Äì Weak and Strong Pointwise Consistency of Graph Laplacians.‚Äù</span> In <em>Lecture Notes in Computer Science</em>, 470‚Äì85. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/11503415_32">https://doi.org/10.1007/11503415_32</a>.
</div>
<div id="ref-kipf2017semisupervised" class="csl-entry" role="listitem">
Kipf, Thomas N., and Max Welling. 2017. <span>‚ÄúSemi-Supervised Classification with Graph Convolutional Networks.‚Äù</span> <a href="https://arxiv.org/abs/1609.02907">https://arxiv.org/abs/1609.02907</a>.
</div>
<div id="ref-Longa2023" class="csl-entry" role="listitem">
Longa, Antonio, Veronica Lachi, Gabriele Santin, Monica Bianchini, Bruno Lepri, Pietro Lio, Franco Scarselli, and Andrea Passerini. 2023. <span>‚ÄúGraph Neural Networks for Temporal Graphs: State of the Art, Open Challenges, and Opportunities.‚Äù</span> <a href="https://arxiv.org/abs/2302.01018">https://arxiv.org/abs/2302.01018</a>.
</div>
<div id="ref-Lu2020" class="csl-entry" role="listitem">
Lu, Jinpeng. 2020. <span>‚ÄúGraph Approximations to the Laplacian Spectra.‚Äù</span> <em>Journal of Topology and Analysis</em> 14 (01): 111‚Äì45. <a href="https://doi.org/10.1142/s1793525320500442">https://doi.org/10.1142/s1793525320500442</a>.
</div>
<div id="ref-michel2023path" class="csl-entry" role="listitem">
Michel, Gaspard, Giannis Nikolentzos, Johannes Lutzeyer, and Michalis Vazirgiannis. 2023. <span>‚ÄúPath Neural Networks: Expressive and Accurate Graph Neural Networks.‚Äù</span> <a href="https://arxiv.org/abs/2306.05955">https://arxiv.org/abs/2306.05955</a>.
</div>
<div id="ref-morris2021weisfeiler" class="csl-entry" role="listitem">
Morris, Christopher, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2021. <span>‚ÄúWeisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks.‚Äù</span> <a href="https://arxiv.org/abs/1810.02244">https://arxiv.org/abs/1810.02244</a>.
</div>
<div id="ref-Nahmias2017" class="csl-entry" role="listitem">
Nahmias, David. 2017. <span>‚ÄúAn Exposition of Spectral Graph Theory.‚Äù</span> <a href="https://dnahmias.com/docs/harmonicfinalpaper.pdf">https://dnahmias.com/docs/harmonicfinalpaper.pdf</a>.
</div>
<div id="ref-Vaswani2017_Transformer" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. <span>‚ÄúAttention Is All You Need.‚Äù</span> In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.
</div>
<div id="ref-veliƒçkoviƒá2018graph" class="csl-entry" role="listitem">
Veliƒçkoviƒá, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and Yoshua Bengio. 2018. <span>‚ÄúGraph Attention Networks.‚Äù</span> <a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a>.
</div>
<div id="ref-Wagstaff2022_UAF_Sets" class="csl-entry" role="listitem">
Wagstaff, Edward, Fabian B. Fuchs, Martin Engelcke, Michael A. Osborne, and Ingmar Posner. 2022. <span>‚ÄúUniversal Approximation of Functions on Sets.‚Äù</span> <em>J. Mach. Learn. Res.</em> 23 (1).
</div>
<div id="ref-xu2018how" class="csl-entry" role="listitem">
Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. <span>‚ÄúHow Powerful Are Graph Neural Networks?‚Äù</span> In <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=ryGs6iA5Km">https://openreview.net/forum?id=ryGs6iA5Km</a>.
</div>
<div id="ref-Zaheer2020_DeepSets" class="csl-entry" role="listitem">
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbhakhsh, Barnab√°s P√≥czos, Ruslan Salakhutdinov, and Alexander J Smola. 2017. <span>‚ÄúDeep Sets.‚Äù</span> In <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 3394‚Äì404. NIPS‚Äô17. Red Hook, NY, USA: Curran Associates Inc.
</div>
</div>
</section>


    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="2023_12_07_MLJC_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>