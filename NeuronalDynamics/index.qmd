---
title: "NeuronalModelling.jl"
subtitle: "Quantitative Single-Neuron Modelling in Julia"
---

<!-- # :clapper: Introduction 

::: {.notes}
...
:::

--- -->

## {.smaller}

### Outline 

<!-- :one: [Objectives](#Objectives)

:two: [Motivations](#Motivations) -->

:one: [Introduction](#Introduction)

:two: [Methods](#Methods)

:three: [Tools](#Tools)

:four: [Results](#Results)

<br>

### Resources 

![](images/logo/JuliaDots.svg){.absolute bottom=252 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [ModellingFramework.jl](https://github.com/InPhyT/ModellingFramework.jl) 

![](images/logo/JuliaDots.svg){.absolute bottom=205 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [NeuronalModelling.jl](https://github.com/InPhyT/NeuronalModelling.jl) 

![](images/logo/GitHub.svg){.absolute bottom=155 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2007)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2007) 

![](images/logo/GitHub.svg){.absolute bottom=107 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2009)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2009) 

![](images/logo/GitHub.svg){.absolute bottom=59 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Allen Brain Atlas (2023)](https://github.com/AllenInstitute/AllenSDK) 

![](images/logo/NeuronalModelling.svg){.absolute bottom=120 right=0 width=45%}

::: {.notes}
...
:::

<!-- https://github.com/quarto-dev/quarto-cli/discussions/4572 -->
```{=html}
<style>
.semi-transparent {
  opacity: 0.3;
}
</style>
```

# :clapper: Introduction 

1.  Objectives 
2.  Motivations

<!-- # :exclamation: Objectives -->

# [Introduction]{.semi-transparent} {.smaller}

1.  Objectives

2.  [Motivations]{.semi-transparent}


## Objectives {.smaller}

:::: {.columns}

::: {.column width="40%"}
### Theory 

Flexible and high-performance computational framework for the specification, calibration and simulation of single-neuron models.

### Application

Electrophysiological time series inference and prediction for individual neurons.
:::

::: {.column width="60%"}
![](images/articles/2009Gerstner.png){.absolute bottom=90 right=-180 width=80%}
:::

::::

::: {.aside}
Figure from [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936).
:::

::: {.notes}
GENERAL / THEORY / METHODOLOGY 

Construction / Development / Implementation of an automatic, high-performance, quantitative single-neuron modelling framework providing a robust specification-calibration-simulation workflow. 

SPECIFIC / APPLICATION / DATA

Single-neuron electrophysiological time series inference and prediction.

REFERENCES 
[Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936).
:::

<!-- # :question: Motivations -->


# [Introduction]{.semi-transparent} {.smaller}

1.  [Objectives]{.semi-transparent}

2.  Motivations

<!-- ## Why Single-Neuron Modelling? {.smaller} -->

## Motivations {.smaller}

- Calibrating neuronal models is a long standing issue ^[[LeMasson and Maex (2001)](https://www.taylorfrancis.com/chapters/edit/10.1201/9781420039290-8/introduction-equation-solving-parameter-fitting), [Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007), [Pospischil et al. (2008)](https://doi.org/10.1007/s00422-008-0263-8), [Druckmann et al. (2008)](https://doi.org/10.1007/s00422-008-0269-2), [Druckmann et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002133), [Toth et al. (2011)](https://doi.org/10.1007/s00422-011-0459-1), [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28), [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615). ];

. . .

- A lot of **hand tuning**  and **domain expertise** is involved in many articles;

. . .

- Currently there is no Julia package that does it fully ^[[JuliaNeuro](https://julianeuro.github.io/), [JuliaNeuroscience](https://github.com/JuliaNeuroscience)].

::: {.notes}
-
-
- At the best of our knowledge there are currently no software packages dedicated to the specification, calibration and simulation of single-neuron models implemented in the Julia language apart from NeuronalModelling.jl itself.
:::



## Motivations {.smaller}

- An international competition has been proposed ^[[Jolivet et al. (2008)](https://doi.org/10.1016/j.jneumeth.2007.11.006), [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x), [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936), [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en), [2007 Training Data](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2007), [2009 Training Data](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2009)];

<!-- ::: {.absolute  top=100 left=800} -->
::: {layout-nrow=1}
![](images/plots/challenge2009input.png){height=200 width=480}
![](images/plots/challenge2009response.png){height=200 width=480}
:::
<!-- ::: -->
. . .

- Best model was heavily hand-tuned ^[[Kobayashi et al. (2009)](https://doi.org/10.3389/neuro.10.009.2009)];

. . .

- Little progress the last 10+ years ^[[AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)].


::: {.notes}
- 

- E' un leaky integrate and fire con una soglia dinamica. Solo 3 parametri liberie fittato unicamente rispetto a coincidence factor.

- 8% di improvement di abdelaty risp a kobayashi, nessuno dal Lynch2015 all'abdelaty2022 ha usato i dati della challenge. Non ci sono articoli degni di nota che tacklino la challenge B.


INTERNATIONAL COMPETITIONS [DESCRIBE WITH MUCH MORE DETAILS AND ADD TO SLIDES!]

- The above challenge has been turned into a single-neuron modeling competition that was first run by Brain Mind Institute at the Ecole Polytechnique Fe ́de ́rale de Lausanne (EPFL) in Switzerland and was officially handed over to the International Neuroinformatic Coordinating Facility (INCF) in Sweden in 2009.
- The idea behind the competition was that a good model can predict neuronal activity based on data that were not adopted in the parameter training / fitting phase. 
- The first spike timing competition in 2007 used an Ornstein-Uhlenbeck current injection with various means and variance to mimic the combined effect of a large number of synapses. In 2008, the competition was modified to replace the dynamic current by dynamic inhibitory and excitatory conductances using dynamic clamp. Then in 2009 the injected current was changed to a current produced by the simulation of six populations of presynaptic neurons changing their firing rate every 200-500 ms.
- Conclusions: High prediction performance is possible in many neuronal systems and **depends strongly on the choice of model structure and calibration method**. One important model feature for high prediction performance is the presence of spike-frequency adaptation. The choice of the model formalism can also influence the fitting method that can be used. High quality prediction is most of the time associated with an efficient and convex fitting method.
- "The data of the challenge will remain available in the future for bench-marking purposes, leaving the possibility for such a deed to be accomplished".
- Among the lessons to be learned from the INCF competition is that every neu- ron is different and one should not think of “the” model of a pyramidal cell or interneuron. Rather, parameters need to be tuned on a neuron-by-neuron basis. Another lesson is that the quality of a neuron model has to be measured on new data that are not accessible during the phase of parameter tuning.

METHODOLOGICAL DEVELOPMENTS 

Many recently (and not so recently) developed calibration / optimisation methods (Bayesian, evolutionary, mono- and multi-objective, etc.) are dramatically under-explored in the theoretical and computational neuroscience literature.

LIMITED PROGRESS IN 10+ YEARS

New optimisation / calibration algorithms have been developed ([AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)) which improved fitting when compared with the old ones between 5 and 8% (coincidence factor).

REFERENCES 

- [Jolivet et al. (2008)](https://doi.org/10.1016/j.jneumeth.2007.11.006) 
- [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x)
- [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936)
- [Rossant et al. (2011)](https://doi.org/10.3389/fnins.2011.00009)
- [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208)
- [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en)
- [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28)
- [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615)
- [Mohacsi et al. (2020)](https://doi.org/10.1109/ijcnn48605.2020.9206692)
- [AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)
:::



<!-- ## hello

![](images/articles/2012Naud.png){.absolute top=78 left=0 width=48%}
![](images/articles/2013Druckmann.png){.absolute bottom=80 left=0 width=48%}
![](images/articles/2011Rossant.png){.absolute top=78 right=0 width=40%}
![](images/articles/2011Naud.png){.absolute bottom=0 right=0 width=45%}

::: {.aside}
<br> <br> <br> <br> 
Figures from [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208), [Rossant et al. (2011)](https://doi.org/10.3389/fnins.2011.00009), <br> [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en) and [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28).
:::

::: {.notes}
WHY NEURONAL MODELLING? / WHY THIS PROJECT? / WHY A SINGLE-NEURON MODELLING FRAMEWORK?  

FOUNDATIONS OF NEUROSCIENCE

- Unraveling and characterising the dynamics of the single neuron, the basic biophysical unit of neural information processing, is one of the most fundamental problems in theoretical, computational and systems neuroscience.
- Single-neuron models often have a large number of free parameters that must be constrained / optimised / estimated. Despite the seemingly straightforward nature of this problem, the main challenge is properly defining and quantifying the match between a given model and a target data set (the similarity / error / distance / loss / cost function).
- Over the last 20 years many researchers have shown that it is not only the temporally averaged firing rate that carries information about the stimulus, but also the exact timing of spikes. Therefore, if spike timing is important, a whole series of questions arises: What is the precision of spike timing if the same stimulus is repeated several times? Do spikes always appear at the same time? What would be a sensible measure of spike timing precision and reliability? Can a neuron model match the spike timing precision of a real neuron? Does it matter which neuron or what stimulus we take? If so, what would be a useful stimulus? More generally: is it possible to predict the spike times of a neuron with millisecond precision?

INTERNATIONAL COMPETITIONS [DESCRIBE WITH MUCH MORE DETAILS AND ADD TO SLIDES!]

- The above challenge has been turned into a single-neuron modeling competition that was first run by Brain Mind Institute at the Ecole Polytechnique Fe ́de ́rale de Lausanne (EPFL) in Switzerland and was officially handed over to the International Neuroinformatic Coordinating Facility (INCF) in Sweden in 2009.
- The idea behind the competition was that a good model can predict neuronal activity based on data that were not adopted in the parameter training / fitting phase. 
- The first spike timing competition in 2007 used an Ornstein-Uhlenbeck current injection with various means and variance to mimic the combined effect of a large number of synapses. In 2008, the competition was modified to replace the dynamic current by dynamic inhibitory and excitatory conductances using dynamic clamp. Then in 2009 the injected current was changed to a current produced by the simulation of six populations of presynaptic neurons changing their firing rate every 200-500 ms.
- Conclusions: High prediction performance is possible in many neuronal systems and **depends strongly on the choice of model structure and calibration method**. One important model feature for high prediction performance is the presence of spike-frequency adaptation. The choice of the model formalism can also influence the fitting method that can be used. High quality prediction is most of the time associated with an efficient and convex fitting method.
- "The data of the challenge will remain available in the future for bench-marking purposes, leaving the possibility for such a deed to be accomplished".
- Among the lessons to be learned from the INCF competition is that every neu- ron is different and one should not think of “the” model of a pyramidal cell or interneuron. Rather, parameters need to be tuned on a neuron-by-neuron basis. Another lesson is that the quality of a neuron model has to be measured on new data that are not accessible during the phase of parameter tuning.

METHODOLOGICAL DEVELOPMENTS 

Many recently (and not so recently) developed calibration / optimisation methods (Bayesian, evolutionary, mono- and multi-objective, etc.) are dramatically under-explored in the theoretical and computational neuroscience literature.

LIMITED PROGRESS IN 10+ YEARS

New optimisation / calibration algorithms have been developed ([AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)) which improved fitting when compared with the old ones between 5 and 8% (coincidence factor).

REFERENCES 

- [Jolivet et al. (2008)](https://doi.org/10.1016/j.jneumeth.2007.11.006) 
- [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x)
- [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936)
- [Rossant et al. (2011)](https://doi.org/10.3389/fnins.2011.00009)
- [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208)
- [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en)
- [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28)
- [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615)
- [Mohacsi et al. (2020)](https://doi.org/10.1109/ijcnn48605.2020.9206692)
- [AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)
:::
 -->


# :bookmark_tabs: Methods 

1.  Models 
2.  Calibration

::: {.notes}
Computational neuroscientists use mathematical models built on observational data to investigate what’s happening in the brain. Models can simulate brain activity from the behavior of a single neuron right through to the patterns of collective activity in whole neural networks. Collecting the experimental data is the first step, then the challenge becomes deciding which computer models best represent the data and can explain the underlying causes of how the brain behaves.

Researchers usually find the right model for their data through trial and error. This involves tweaking a model’s parameters until the model can reproduce the data of interest. But this process is laborious and not systematic. Moreover, with the ever-increasing complexity of both data and computer models in neuroscience, the old-school approach of building models is starting to show its limitations.
:::

# [Methods]{.semi-transparent} {.smaller}

1.  Models:

    2.1 Leaky Integrate and Fire

    2.2 Hodgkin-Huxley

    2.3 Two Dimensional Reduction

    2.4 One Dimensional Reduction

    2.5 Exponential NLIF

    2.6 AdEx-NLIF

2.  [Calibration]{.semi-transparent}

## Models: Leaky integrate and fire {.smaller}

- Subthreshold dynamics:
$$
\tau \frac{du}{dt} = -(u(t) - u_{rest}) + RI(T)
$${#eq-lif}

![](images/books/2014Gerstner-LIF_circuit.png){.absolute top=90 left=850 width=25%}

- With firing mechanism: 

$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$${#eq-lif_reset}

. . .

- Cannot describe Bursting, Adaptation, Delayed Response, Inhibitory Rebound etc.

. . .


- Cannot describe spike features such as spike height, width at half height, etc...

::: {.notes}
Comes from considering the neuron as the circuit in fig A.
:::

## Models: Hodgkin-Huxley {.smaller}

$$
C\frac{du}{dt} = -\sum\limits_k I_k(t) + I(t)
$${#eq-hh_u}

. . .


Where, in the simplest case ^[[Gerstner et al. (2014)](https://neuronaldynamics.epfl.ch/online/Ch2.S2.html)]:

$$
\sum\limits_k I_k(t) = g_{Na}m^3h(u - E_{Na}) + g_Kn^4(u-E_K) + g_L(u-E_L)
$${#eq-hh_currents}

. . .

Where, setting $x \in \{m,n,h\}$:

$$
\dot{x} = \alpha_x(u)(1-x) - \beta_x(u)x
$${#eq-gating_variable_exp_diffeq}

. . .

Where, for instance:

$$
\alpha_m(u) = \frac{0.182(u+35)}{1-e^\frac{u+35}{9}} \; , \; \beta_m(u)= \frac{-0.124(u+35)}{1-e^\frac{u+35}{9}}
$${#eq-boltzmann_terms_exp}

## Models: Hodgkin-Huxley {.smaller}

**Unstable** $\rightarrow$ define^[[Toth et al. (2011)](https://doi.org/10.1007/s00422-011-0459-1)]:

$$
x_0(u) := \frac{\alpha_x(u)}{\alpha_x(u) +  \beta_x(u)} = \frac{1}{2}\left(1+\tanh\left(\frac{u - u_x}{\kappa_x}\right)\right)
$${#eq-x_0}
$$
\tau_x(u) = \frac{1}{\alpha_x(u) +  \beta_x(u)} = \tau^0_x + \tau^{max}_x\left(1-\tanh\left( \frac{u - u_{\tau,x}}{\sigma_x} \right)^2\right)
$${#eq-tau}

. . .

And then substitute @eq-gating_variable_exp_diffeq with:

$$
\dot{x} = \frac{x_0(u) - x(u)}{\tau_x(u)}
$${#eq-gating_variables_tanh_diffeq}

. . .

$\rightarrow$ solver stability.

::: {.notes}
The exponential form is numerically unstable → we use the $\tanh$ formalism 1. We define:

. . .

Parameters values must then be fitted to the shapes of @eq-boltzmann_terms_exp

. . .

Which achieves solver stability.
:::

## Models: two dimensional reduction {.smaller}

Hodgkin-Huxley models are a bit **unpractical**:

- No voltage nor current thresholds:

::: {layout-nrow=1}
![](images/books/2014Gerstner-no_voltage_threshold.png){width=500 height=300}
![](images/books/2014Gerstner-no_current_threshold.png){width=500 height=300}
:::

. . .

- Impossible to visualize and analyze a 4d+ dynamical system;

. . .

- Best of both worlds $\rightarrow$ **dimensionality reduction**!

::: {.notes}
- Usually required for intuitiveness
- 
- replication of all firing patterns & well-defined threshold, simpler system, parameter identifiability

To these, we'd also add a certain increase in the nuance required to implement and calibrate HH systems w.r.t. NLIF that we're about to see.
:::

## Models: two dimensional reduction {.smaller}

Note: dynamics of $n$ and $h$ are very slow w.r.t. $m$

![](images/books/2014Gerstner-tau_x){.absolute top=90 left=650 width=40%}

. . .


::: {.absolute top=250}
$\rightarrow$ define $:w = b-h = an$ 

$\rightarrow$ set: $m(u) = m_0(u)$


::: {.fragment}
So that @eq-hh_u becomes: 
$$
\frac{du}{dt} = \frac{1}{\tau}(F(u,w) + RI(t))
$$
:::

::: {.fragment}
And @eq-gating_variable_exp_diffeq (or @eq-gating_variables_tanh_diffeq) reduce to: 
$$
\frac{dw}{dt} = \frac{1}{\tau_w}G(u,w)
$$
:::

::: {.fragment}
Examples: Morris-Lecar model, FitzHugh-Nagumo model
:::
:::


## Models: one dimensional reduction {.smaller}

Define $\epsilon := \frac{\tau}{\tau_w}$. If $\tau_w \gt \gt \tau$:

![](images/books/2014Gerstner-one_dimensional_reduction_firing_mechanism)<!-- {.absolute top=90 left=650 width=40%} -->

. . .

::: {.absolute top=80 left=430}
Since:

- One may **NOT** interested in spike shape;

::: {.fragment}
- The $w$ variable only varies **after** the voltage has reached its maximum.
:::
:::

. . .

$\rightarrow$ substitute the equation for $\frac{dw}{dt}$ with reset mechanism! $\rightarrow$ **NLIF** models:

$$
\frac{du}{dt} = \frac{1}{\tau}(F(u,w_{rest}) + RI(t)) =: \frac{1}{\tau}(f(u) + RI(t))
$${#eq-NLIF} 
$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$${#eq-one_dimensional_reduction}



::: {.notes}
Excitability can now be discussed with the help of Fig. 4.21. A current pulse shifts the state of the system horizontally away from the stable fixed point. If the current pulse is small, the system returns immediately (i.e., on the fast time scale) to the stable fixed point. If the current pulse is large enough so as to put the system beyond the middle branch of the u-nullcline, then the trajectory is pushed toward the right branch of the u-nullcline. The trajectory follows the u-nullcline slowly upward until it jumps back (on the fast time scale) to the left branch of the u-nullcline. The ‘jump’ between the branches of the nullcline corresponds to a rapid voltage change. In terms of neuronal modeling, the jump from the right to the left branch corresponds to the downstroke of the action potential. The middle branch of the u-nullcline (where u˙>0) acts as a threshold for spike initiation;

. . .
- they're roughly all the smae -> they carry no information
-
. . .

w_rest is the w at the intersection of the nullclines. Below is the firing mechanism. $\delta$ can be interpreted as the "dead time"
:::

## Models: Exponential NLIF {.smaller}

How to choose $f(u)$?

. . .

![](images/books/2014Gerstner-AdEx_derivation){height=200}

::: {.absolute top=150 left=550}
$$
\frac{f(u(t))}{\tau} = \left<\frac{I(t)}{C} - \frac{du}{dt}\right>
$$
:::

. . .

$\rightarrow$ best fitting function is: $f(u)= -(u-u_{rest}) + \Delta_T e^{\frac{u-\theta_{rh}}{\Delta_T}}$

. . .

<!-- Would get the same result by approximating the $F(u,w_{rest})$ of a 2-dimensional Hodgkin-Huxley model: -->

![](images/books/2014Gerstner-AdEx_derivation_hh){height=200}

::: {.absolute top=500 left=550}
Same result from reduced 1d Hodgkin Huxley model.
:::

::: {.notes}
By performing experiments:

In order to determine the function f~(u), an experimentalist injects a time-dependent current I(t) into the soma of a neuron while measuring with a second electrode the voltage u(t). From the voltage time course, one finds the voltage derivative du/dt.

A measurement at time t yields a value u(t) (which we use as value along the x-axis of a plot) and a value [(I(t)/C)−(du/dt)] (which we plot along the y axis). With a thousand or more time points per second, the plot fills up rapidly. For each voltage u there are many data points with different values along the y-axis. The best choice of the parameter C is the one that minimizes the width of this distribution. At the end, we average across all points at a given voltage u to find the empirical function (35)

. . .

We find that the empirical function extracted from experiments is well approximated by a combination of a linear and exponential term

. . .

In figure you see a plot of the $F(u, {w_rest})$ of a reduced to 2d HH model where we make the further assumption to make it 1d (separation of time scales). There a 2 fixed points, the leftmost is stable (u_rest), the one in the middle is unstable and denotes the effective threshold. The rightmost one ends the upswing of the spike. One could discard the rightmost and introduce a numerical threshold $\theta_{reset}$ together with a reset mechanism. This way, what's left of the $F(u, {w_rest})$ can be approximated with the $f(u)$ of the AdEx above.

:::


## Models: AdEx-NLIF {.smaller}

A single @eq-NLIF cannot capture all firing patterns $\rightarrow$ add abstract **linear** current:

$$
\tau\frac{du}{dt} =f(u) -R\sum\limits_k w_k + RI(t) \; , \; \{\tau_k\frac{dw_k}{dt} = a_k(u-u_{rest}) -w_k +b_k\tau_k\sum\limits_{t^{(f)}}\delta(t-t^{(f)})\}
$$
$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$$

. . .

![](images/books/2014Gerstner-firing_patterns){height=300 width=800}

::: {.notes}
The δ-function in the wk equations indicates that, during firing, the adaptation currents wk are increased by an amount bk.The parameters bk  are the ‘jump’ of the spike-triggered adaptation.

. . .

Spike-triggered adaptation is controlled by a combination of a and b. The choice of a and b largely determines the firing patterns of the neuron. In figure, you may see al firing patterns as a function of a and b. FOr every pattern, an hight and a low step current are reported.
:::

# [Methods]{.semi-transparent} {.smaller}

1.  [Model Specification]{.semi-transparent}
2.  Model Calibration:

    3.1 Intro

    3.2 Objective Functions

    3.3 Feature selection
    
    3.4 MOO

    3.5 BORG-MOEA

    3.6 Pareto-optimal front

## Calibration: Intro

-   **Model calibration** find a parameter set $\vec{w}$ that makes the model *match* experimental trace;

. . .

-   The notion of "matching" will be made precise by the definition of **objective functions** (**loss**);

. . .

-   Calibration methods will need to take into account the **intrinsic stochasticity** of neurons.

:::{.notes}
- "Model calibration" refers to the task of finding a parameter set $\vec{w}$ that makes the model *match* experimental trace;
- i.e. functions that measure the degree to which the model is capable of reproducing experimental traces along certain dimensions;
- i.e. their tendency to produce different voltage traces when injected with the same input.
:::

## Calibration: Objective Functions {.smaller}

Possible candidates are **Trace-to-trace (direct) comparisons**:

-   L2 loss;
-   Phase plane trajectory distance ^[[LeMasson and Maex (2001)](https://www.taylorfrancis.com/chapters/edit/10.1201/9781420039290-8/introduction-equation-solving-parameter-fitting)];
-   ...

. . .

These usually ^[We haven't yet considered more sophisticated approach like [Brookings et al. (2014)](https://doi.org/10.1152/jn.00007.2014)] perform poorly, since:


-   Sub- and supra-threshold dynamics are *unbalanced*;

. . .

-   Small *shifts* between simulated and measured spikes usually yield large *L2* errors.

. . .


-   *Intrinsic stochasticity* makes the selection of the experimental repetition $r \in R$ to match *arbitrary*;


::: notes
-
-
-


-   True bith for L2 and Lemasson's loss

-   This can be corrected by using the Lemasson and Maex's loss, but one loses all temporal information with it;

-   
:::

## Calibration: Objective Functions

$\rightarrow$ use **Feature-based distance functions** [^2]:

- Firing frequency;
- Spike height;
- Latency;
- Accomodation index;
- Coincidence factor;
- Slope of the fit of the spike peaks;
- ...

## Calibration: Objective Functions {.smaller}

They avoid the previous issues, because

-   They are not point-wise, thus insensitive to *regime imbalances*;

. . .

-   They directly make the model reproduce *qualitative* features of the target neuron;

. . .

-   They take into account *Intrinsic stochasticity*:

. . .

By defining the loss relative to feature $f$ w.r.t. repetition set $R$ :  $$L_f(\vec{w}, R) = \frac{|\bar{f}_R - f(s(\vec{w}))|}{\sigma^f_R}$$

<!-- $L_f(\vec{w}, R)$   *Intrinsic stochasticity* within repetition set $R$ w.r.t. feature $f$ can be dealt with by defining the loss: -->

. . .

- We say that we have a good fit when every objective is under 2 SD ^[[Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)].


[^2]: [Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)

::: notes
-   

-   

Where:

-   $R$ is a set of voltage traces produced by the same neuron injected with the same current
-   $\vec{w}$ is a set of model parameters
-   $s(\vec{w})$ is a voltage trace simulated by the model with parameters $\vec{w}$
-   $\bar{f}_R$ is the average value of feature $f$ over $R$
-   $\sigma^f_R$ is the standard deviation of $f$ over $R$
-   $f(s(\vec{w}))$ is the value of feature $f$ over $s(\vec{w})$
:::

## Calibration: Feature selection {.smaller .scrollable}

Experiments ^[[Druckmann et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002133)] suggest the following associations:

. . .

- Square inputs:

  - Firing frequency;
  - Latency;
  - Accomodation index;
  - Width of AP at half height (only for HH models);
  - Average height of AP;
  - Slow throughs;

. . .

- Ramp inputs:

  - Average height of AP;
  - Slow throughs;
  - Slope of the fit of the spike peaks;

. . .

- Colored noisy inputs:

  - Coincidence factor;
  - Control-adjusted RMS ^[[Brookings et al. (2014)](https://doi.org/10.1152/jn.00007.2014)].



## Calibration: MOO

> How to combine the different features?

. . .

**Weighted sum**? No:

-   Choice of weights is not obvious;

. . .

-   No guarantee on the nature and quality of the compromise;

. . .

$\rightarrow$ **Multi-Objective Optimization** !


## Calibration: MOO {.smaller}

**Def.** Given the objectives $\{L_{f_1}, L_{f_2}, ..., L_{f_n}\}$, a set of repetitions $R$ and two parameter sets $\vec{w_1}$ and $\vec{w_2}$ we say that $\vec{w_1}$ **dominates** $\vec{w_2}$ $\iff$:

1.  $L_{f_i}(\vec{w_1},R) \leq L_{f_i}(\vec{w_2},R) \qquad \forall i= 1,...,n$
2.  $\exists \bar{i} \in \{1,..,n\} | L_{f_\bar{i}}(\vec{w_1},R) \lt L_{f_\bar{i}}(\vec{w_2},R)$

. . .

MOO purpose: find the set of all $\vec{w}$ that do not dominate each other.

. . .

Such set is the **Pareto-optimal front** (**POF**).


## Calibration: BORG-MOEA

- Calibration algorithm: [BORG-MOEA](http://borgmoea.org/);

. . .

- It is an evolutionary algorithm with self-tuning capabilities;

. . .

- Pitting it against other algorithms determined its advantage.



## Calibration: Pareto-optimal front #1 {.smaller}

- The purpose of a *MOEA* is to approximate the *POF* by finding some of its elements;

. . .

- Projection of the *POF* set may assume two configurations:

![Projections of the Pareto optimal front along two sets of features ^[[Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)]](images/articles/2007Druckmann_POF.png){height=200 width=800}

. . .

- First configuration $\rightarrow$ the two objectives do not conflict each other;

. . .

- Second configuration $\rightarrow$ the two objectives do conflict each other;


## Calibration: Pareto-optimal front #2

- Calibration algorithms work better the less objectives they have;

. . .

- $\rightarrow$ one may think to sum together features repeated across stimuli;

. . .

- Only possible if the two features are **NOT** in a tradeoff ^[[Druckmann (2013)](https://doi.org/10.1007/s00422-008-0269-2)]


:::{.notes}
-
-
- In fact if they are not in a tradeoff the calibration algorithm, can minimize the sum of the two. Instead if they are in a tradeoff, if we sum them the algorithm will find a compromise whose nature and quality we're not guaranteed of 8we'd be bak to the problem in the MOO #1 slide.
:::


## Calibration: Pareto-optimal front #3

- Configuration of the projections of the POF are not known a priori $\rightarrow$ features must be kept separate;

. . .

- To anyway reduce the number of features, one may resort to objectives combinations ^[[Hay et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002107)];

. . .

- They consists in substituting two features with the maximum or other combinations of the two. 

::: notes
:::


# :computer: NeuronalModeling.jl 

1. Why Julia?
2. Overview
3. Demonstration
4. Future developments

::: {.notes}
:::


# [NeuronalModelling.jl]{.semi-transparent}

1. Why Julia?

    1.1 Performance;

    1.2 Metaprogramming;

    1.3 Simple Workflow;

    1.4 SciML.

2. [Overview]{.semi-transparent}
3. [Demonstration]{.semi-transparent}
4. [Future developments]{.semi-transparent}


## Why Julia? {.smaller}

![](images/articles/2023Roesch-1.webp){.absolute bottom=80 left=0 width=50%}
![](images/articles/2023Roesch-2.webp){.absolute bottom=120 right=-200 width=65%}

::: {.aside}
Figures from [Roesch et al. (2023)](https://doi.org/10.1038/s41592-023-01832-z).
:::

::: {.notes}
WHY JULIA? 

- Julia solves the two-language problem: users do not have to choose between ease of use and high performance. 
- Julia is user friendly. It is easy to code.
- Julia is a high performance language. It is fast.
- Julia offers a high level of abstraction. It is flexible.
- Julia can be used for metaprogramming. It can code automatically.
- Julia is not only good in one area but in many. It enables “one language” projects.
- Easy to learn due to intuitive semantics and easy to read syntax.
- Accessible via various interfaces, REPL, IDE, or Jupyter notebook.
- Existing non-Julia code can be easily integrated into new Julia projects via language specific packages (interoperability).
- Julia is free, open-source and hosted on GitHub.
- Julia offers (generally) excellent documentation, tutorials, and help available directly from active and welcoming community members via various communication channels such as Slack, Discourse, Twitter or Zulip.
- Julia’s package ecosystem provides functionality for a wide range of oft-performed tasks in computational biology research.
- Julia code is smoothly extendable which enables and encourages easy contributions and collaborations to/with existing projects, as well as writing, integrating and sharing new, user specific packages.

REFERENCES

[Roesch et al. (2023)](https://doi.org/10.1038/s41592-023-01832-z).
:::
    
    
## Why Julia?: Performance


*Julia writes like Python but executes as fast as C...*

::: {.absolute top=300}

::: {.fragment}

*...assuming you know how to use it.*^[[Holy (2022)](https://www.youtube.com/playlist?list=PL-G47MxHVTewUm5ywggLvmbUCNOD2RbKA)]

:::
:::

---

## Why Julia?: Performance {.smaller}
One **function** $\rightarrow$ several **methods**:

```{.julia}
methods(+)[1:3] # 206 total methods at the time of writing
```
```{.nothing}
[1] +(x::T, y::T) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} in Base at int.jl:87
[2] +(x::T, y::T) where T<:Union{Float16, Float32, Float64} in Base at float.jl:383
[3] +(c::Union{UInt16, UInt32, UInt8}, x::BigInt) in Base.GMP at gmp.jl:531     
```

. . .

**Method selection**:

1. (Arguments') Type Inference;

2. Method lookup.



::: {.notes}
In programming, usually each function has several methods, one per combination of its arguments' types: 

. . .

Whenever a function is called with certain arguments, the right method has to be selected. The method selection has two parts:
:::

## Why Julia?: Performance {.smaller}

**Type Inference**:

```{.julia}
julia> @code_warntype(loss_square110pA(Vector{Float64}(initial_parameters_values_AdEx[2:end])))
MethodInstance for (::RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:theta,), NeuronalModeling.var"#_RGF_ModTag", NeuronalModeling.var"#_RGF_ModTag", (0xb92b619f, 0x78b8fd2f, 0xcb257d79, 0xdca519fc, 0xc02e6bbc), Expr})(::Vector{Float64})
  from (f::RuntimeGeneratedFunctions.RuntimeGeneratedFunction)(args::Vararg{Any, N}) where N @ RuntimeGeneratedFunctions C:\Users\claud\.julia\packages\RuntimeGeneratedFunctions\mfDmv\src\RuntimeGeneratedFunctions.jl:125
Static Parameters
  N = 1
Arguments
  f::RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:theta,), NeuronalModeling.var"#_RGF_ModTag", NeuronalModeling.var"#_RGF_ModTag", (0xb92b619f, 0x78b8fd2f, 0xcb257d79, 0xdca519fc, 0xc02e6bbc), Expr}
  args::Tuple{Vector{Float64}}
Body::NTuple{7, Float64}
1 ─ %1 = RuntimeGeneratedFunctions.generated_callfunc::Core.Const(RuntimeGeneratedFunctions.generated_callfunc)
│   %2 = Core.tuple(f)::Tuple{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:theta,), NeuronalModeling.var"#_RGF_ModTag", NeuronalModeling.var"#_RGF_ModTag", (0xb92b619f, 0x78b8fd2f, 0xcb257d79, 0xdca519fc, 0xc02e6bbc), Expr}}
│   %3 = Core._apply_iterate(Base.iterate, %1, %2, args)::NTuple{7, Float64}
└──      return %3
```

. . .

(Successful) Type Inference $\rightarrow$ Method Lookup at compile time

::: {.notes}

/

. . .

If successful, type inference at compile time will also trigger method lookup at compile time, which has the potential to speed up code by orders of magnitude.

:::



## Why Julia?: Metaprogramming {.smaller}

Example from the manual:

```{.julia}
for op = (:sin, :cos, :tan, :log, :exp)
    eval(quote
        Base.$op(a::MyNumber) = MyNumber($op(a.x))
    end)
end
```
. . .

Example from NeuronalModeling.jl:

```{.julia}
function borg_moea_loss_closure(hh_model::H, 
                                # More arguments...
)
    # Code to distinguish feature aggregation...

    # We need to generate the loss at runtime since, ar compile time, `num_features`is not known causing it to be not type-stable
    loss_expr = :( 
        
    function borg_moea_loss(theta::Vector{Float64})::NTuple{$num_features,Float64}

        losses_values_by_feature_dict = hh_neuron_loss_intermediate($hh_model, theta, $fixed_parameters_indexes, $variable_parameters_indexes, deepcopy($fixed_parameters_values), $save_idxs, $filter, $resting_potentials_values_by_type_amplitude, $features_targets_lossKwargs_by_type_amplitude, $opt_trim_associations, $fast_problems_by_type_amplitude_dict, $time_steps_by_type_amplitude_dict, $tstops_by_type_amplitude_dict, $features_counts_dict, $electrophysiology_backend, $aggregation_strategy_of_same_feature_across_different_type_amplitudes)

        return NTuple{$num_features,Float64}([loss_val <= 1000.0 ? loss_val : 1000.0 for loss_val in values(losses_values_by_feature_dict)]) # length(losses_values_by_feature_dict)
    end
    )

    return @RuntimeGeneratedFunction(loss_expr)
end
```

::: {.notes}
It refers to the set of features that allows a program to generate and execute parts of its code at run time:
:::


## Why Julia?: Simple Workflow {.smaller}

It is *easy* to contribute a package to the Julia's ecosystem:

1. `generate` the package's structure via PkgTemplates.jl;

. . .

2. Write the code;

. . .

3. Publish the repository on GitHub;

. . .

4. Invoke `@JuliaRegistrator register` in an issue.

. . .

```{.julia}
add YourPackage
```

::: {.notes}
1.
2.
3.
4.

The final command will open a PR to the JuliaRegistries/General public repository, which will make `YourPackage.jl` available for everyone to be installed via: 
:::


## Why Julia? SciML

It is an **ecosystem** featuring:

- [DifferentialEquations.jl](https://github.com/SciML/DifferentialEquations.jl);

. . .

- [ModelingToolkit.jl](https://github.com/SciML/ModelingToolkit.jl);

. . .

- [Optimization.jl](https://github.com/SciML/Optimization.jl);

. . .

- [DiffeqFlux.jl](https://github.com/SciML/DiffEqFlux.jl), [NeuralPDE.jl](https://github.com/SciML/NeuralPDE.jl).

::: {.notes}
- A state of the art differential equations suite;
- Symbolic optimization & modeling;
- Suite of optimization algorithms;
- Advanced functionalities (scientific machine learning)
:::

# [NeuronalModelling.jl]{.semi-transparent}

1. [Why Julia?]{.semi-transparent}

2. Overview:

    2.1 Model specification; 

    2.2 Data;

    2.3 Features;

    2.4 Associations;

    2.5 Calibration Pattern;

3. [Demonstration]{.semi-transparent}
4. [Future developments]{.semi-transparent}


## Overview: Model Specification #1

Nonlinear integrate-and-fire models are specified via the `IFModel` struct^[[Balasubramanian](http://indico.ictp.it/event/a13235/session/99/contribution/371/material/0/1.pdf)]:

```{.julia}
mutable struct IFModel <: AbstractIFModel 
    name::String
    deterministic_dynamic::Function
    IC_evaluation_function::Function
    calibration_history::Calibration_History
    firing_mechanism::Union{DiscreteCallback,ContinuousCallback}
    model_kwargs::NamedTuple{<:Any, <:Tuple{Vararg{Any}}}
end
```

::: {.notes}
- The `deterministic_dynamics` is the system of differential equations describing the neuron;

- The `IC_evaluation_function` is the function that, given parameters values, initializes the model;

- The `firing_mechanism` specifies the firing behaviour;

- The `calibration_history` is a record of all calibration steps. Informally, it keeps track of all parameters' values and model posterior at every stage of the calibration (we will deal with it later);
:::

## Overview: Model Specification #2 {.smaller}
Example (AdEx):

```{.julia}
# AdEx parameters names. "Dead time" missing due to issue in DifferentialEquations.jl (see links below)
const parameters_names_AdEx = ["spike_times", "V_th", "u_r", "b", "C", "g_L", "u_rest", "Δ_T", "θ_rh", "a", "τ_w"]

# Initial parameters values for the AdEx model
const initial_parameters_values_AdEx_dict = OrderedDict("spike_times" => Float64[], 
                                                        "V_th" => -60.0, # V_th (mV)
                                                        "u_r" => -71.0 , #  u_r (mV)
                                                        "b" => 60e-6, #  b (μA)
                                                        "C" =>  (1 / ( 20e-3 * 500e6) )*1e4,  # C = 1/ (τ_m * R) (μF)
                                                        "g_L" => (1/(500e6)) * 1e3, #  g_L = 1/R  (mS)
                                                        "u_rest" => -70.0 ,  #  u_rest (mV)
                                                        "Δ_T" => 2.0, #  Δ_T (mV)
                                                        "θ_rh" => -65.0, # θ_rh (mV)
                                                        "a" => 0.0 * 1e-6,  #  a (mS)
                                                        "τ_w" => 30.0) #  τ_w (msec)  taken from table 6.1 https://neuronaldynamics.epfl.ch/online/Ch6.S2.html , tonic



# Function that computes the initial conditions for the AdEx model given the parameters values (cfr Future developments: Hodgkin-Huxley model)
get_AdEx_IC(V_rest::Float64, all_parameters_values::Vector{<:Any}) = [V_rest, 0.0]
    

# Deterministic dynamics for the AdEx. Implementation features have been chosen via benchmarking multiple approaches
function AdEx_deterministic_dynamic!(input::F) where {F <: Function}
    function parametrized_AdEx_deterministic_dynamic!(du, u, p, t)
        
        spike_times, V_th, u_r, b, C, g_L, u_rest, Δ_T, θ_rh, a,  τ_w = p 

        # # State variables
        V = @view u[1]
        w = @view u[2]

        # State variables
        dV = @view du[1]
        dw = @view du[2]

        dV .= (1 ./C) .* ( (-1) .* g_L .* (V .- u_rest) .+ g_L .* Δ_T .* exp((V .- θ_rh) ./ Δ_T) .- w .+ input(t))
        dw .= (1 ./ τ_w) .* (a .* (V .- u_rest)  .- w)

    end
end

# Firing mechanism
function AdEx_firing_callback_discrete_condition(u,t,integrator)
    u[1] >= integrator.p[2]
end

function AdEx_firing_callback_affect!(integrator)
    push!(integrator.p[1], integrator.t)
    integrator.u[1]  = integrator.p[3]
    integrator.u[2] += integrator.p[4]
end


const AdEx_firing_discrete_callback = DifferentialEquations.DiscreteCallback(AdEx_firing_callback_discrete_condition, AdEx_firing_callback_affect!, save_positions = (false,false))
```

::: {.aside}
[Issue](https://github.com/SciML/DifferentialEquations.jl/issues/868), [Discourse](https://discourse.julialang.org/t/error-when-trying-to-dynamically-change-tstops-during-solve/81999)
:::

## Overview: Model Specification #3 {.smaller}

So that the model can be instantiated as:

```{.julia}
# Parameters' priors are defined as Uniform distribution around the values provided in `initial_parameters_values_AdEx_dict`
parameters_priors_AdEx = get_uniform_priors(initial_parameters_values_AdEx, range = 1 )
# Define calibratable and non-calibratable parameters
parameters = [name != "spike_times" ? Parameter(name, val, prior,true) : Parameter(name, val, prior,false) for (name, val, prior) in zip(parameters_names_AdEx, initial_parameters_values_AdEx, parameters_priors_AdEx)]
# Outer constructor
AdEx = IFModel( "Adaptive Exponential",
                AdEx_deterministic_dynamic!,
                get_AdEx_IC,
                parameters,
                AdEx_firing_discrete_callback 
);  
```




## Overview: Data #1

- Data were taken from the [Allen Brain Atlas](https://celltypes.brain-map.org/);

. . .

- Data: voltage recordings of cell responses to input currents (square, multiple square, impulse, ramp, white noise, etc); 

. . .

- We selected [one cell](http://celltypes.brain-map.org/mouse/experiment/electrophysiology/488697163) in particular because of its characteristics;


::: {.notes}
-
- Data consists of voltage recordings of membrane potential responses to multiple types of input current (square, multiple square, impulse, ramp, white noise, etc) at different amplitudes injected in various cells;
- 
:::

## Overview: Data #2 {.smaller}

[AllenSDK](https://allensdk.readthedocs.io/en/latest/) provides automated download and caching of the data from Allen Brain Atlas.

. . .

API for importing data from Allen website:
```{.julia}
cell_dataset = get_cell_dataset(manifest_path,488697163)
cell_dataset["data"]
```
```{.nothing}
Dict{String, Dict{String, Type_Amplitude}} with 8 entries:
  "ramp"                        => Dict("800.0 pA"=>Type_Amplitude…
  "square - 2s suprathreshold"  => Dict("190.0 pA"=>Type_Amplitude…
  "test"                        => Dict("0.0 pA"=>Type_Amplitude…
  "long square"                 => Dict("-110.0 pA"=>Type_Amplitude…
  "square - 0.5ms subthreshold" => Dict("-200.0 pA"=>Type_Amplitude…
  "noise 2"                     => Dict("259.375 pA"=>Type_Amplitude…
  "short square"                => Dict("800.0 pA"=>Type_Amplitude…
  "noise 1"                     => Dict("255.0 pA"=>Type_Amplitude…
```

. . .

Access recordings relative to particular input type:
```{.julia}
cell_dataset["data"]["square - 2s suprathreshold"]
```
```{.nothing}
Dict{String, Type_Amplitude} with 3 entries:
  "190.0 pA" => Type_Amplitude…
  "150.0 pA" => Type_Amplitude…
  "110.0 pA" => Type_Amplitude…
```

:::{.notes}
Our pipeline automatically converts Allen's rather involved data structures in a dictionary of sweeps organized by type and amplitude of the input current:
:::

## Overview: Data #3 {.smaller}

Each input type and amplitude has usually been repeated more than once. We collect the corresponding sweeps in a dedicated `Type_Amplitude` struct:

```{.julia}
struct Type_Amplitude <: AbstractMeasurement
    type::String
    amplitude::String
    sweeps::Tuple{Vararg{Sweep}}
    interpolated_input_closure::Function
end
```
. . .

Example:

```{.julia}
cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"]
```
```{.nothing}
Type_Amplitude
┌────────────────────────────┬───────────┬──────────────────┬────────────────────────────────────────────┐
│            TYPE            │ AMPLITUDE │ NUMBER OF SWEEPS │        NAME OF INPUT INTERPOLATION         │
├────────────────────────────┼───────────┼──────────────────┼────────────────────────────────────────────┤
│ square - 2s suprathreshold │ 110.0 pA  │        4         │ square__2s_suprathreshold_110_0_pA_closure │
└────────────────────────────┴───────────┴──────────────────┴────────────────────────────────────────────┘
```

## Overview: Data #4 {.smaller}

Every repetition of the same input is called `Sweep`. Thus, `Sweep`s contain the raw data, i.e. the time series of input current and voltage response:

```{.julia}
struct Sweep{T}
    name::String
    v::Vector{T}
    i::Vector{Float64}
    t::Vector{Float64}
    index_range::Tuple{Int64,Int64}
    UOM::NamedTuple{(:v, :i, :t),Tuple{Float64,Float64,Float64}}
    extra_data::NamedTuple{<:Any,<:Tuple{Vararg{Any}}}
end
```

. . .

Example: 
```{.julia}
cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"].sweeps[1]
```
```{.nothing}
SWEEP Square__2s_Suprathreshold_110_0_pA
┌───────────────────────┬────────────────────────────────────┐
│       PROPERTY        │               VALUE                │
├───────────────────────┼────────────────────────────────────┤
│      index_range      │         (150000, 2004000)          │
├───────────────────────┼────────────────────────────────────┤
│ Units of Measure (SI) │ (v = 0.001, i = 1.0e-6, t = 0.001) │
└───────────────────────┴────────────────────────────────────┘
```

## Overview: Data #4 {.smaller}

We may visualize the sweeps corresponding to different input currents:

::: {layout-nrow=2}
![](images/plots/square_110pA_sweeps.png){width=45% height=250}  <!--# .absolute bottom=90 right=-20 -->
![](images/plots/ramp_800pA_sweeps.png){width=45% height=250} 
![](images/plots/square_110pA_input.png){width=45% height=250} 
![](images/plots/ramp_800pA_input.png){width=45% height=250} 
:::


## Overview: Features #1 <!-- {.smaller} -->

Features are implemented via the `ElectrophysiologicalFeature` construct:

```{.julia}
mutable struct ElectrophysiologicalFeature <: AbstractElectrophysiologicalFeature
    name::String
    UOM::Float64
    target_evaluation_on_sweep::Function
    target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude::Function
    loss::Function
    loss_kwargs::NamedTuple{<:Any, <:Tuple{Vararg{Any}}}
    target_lossKwargs_evaluation_kwargs::NamedTuple{<:Any, <:Tuple{Vararg{Any}}}
end
```

::: {.notes}
Features at `Sweep` and `Type_Amplitude` level are implemented via the `ElectrophysiologicalFeature` construct:
:::

## Overview: Features #2 {.smaller}

Example of `Sweep` level feature, the Firing Frequency:

```{.julia}
# Evaluation on NLIF model simulation
allen_firing_frequency_Hz_evaluation_on_sweep(sol::OrdinaryDiffEq.ODESolution) = length(sol.prob.p[1]) / (diff(vcat(sol.prob.tspan...))[1] * 1e-3)

# Evaluation on electrophysiological recordings
allen_firing_frequency_Hz_evaluation_on_sweep(ephys_sfe::PyObject) = ephys_sfe.sweep_feature("avg_rate")

# Target and loss kwargs evaluation
function allen_firing_frequency_Hz_target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude(ephys_sfes::Vector{PyObject})

    # Evaluate firing frequency for all electrophysiological recordings
    sweep_features = allen_firing_frequency_Hz_evaluation_on_sweep.(ephys_sfes)

    # As per the literature, the target is the mean of the sweep-level features
    target = mean(sweep_features)

    # As per the literature, the loss must be normalized by the experimental standard deviation
    experimental_standard_deviation = (Statistics.std(sweep_features))

    # If the experimental std is lesser than a certain threshold (e.g. only one sweep is available), then use an empirical value stored elsewhere
    standard_deviation = isnan(experimental_standard_deviation) || experimental_standard_deviation <  allen_default_standard_deviations["firing_frequency_Hz"] ? allen_default_standard_deviations["firing_frequency_Hz"] : experimental_standard_deviation

    return target, (standard_deviation = standard_deviation,), ()

end

# Define the feature
allen_firing_frequency_Hz() = ElectrophysiologicalFeature(  
                                    "allen_firing_frequency_Hz",
                                    1e0,
                                    allen_firing_frequency_Hz_evaluation_on_sweep,
                                    allen_firing_frequency_Hz_target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude,
                                    druckmann_loss;
)
```

## Overview: Features #3 {.smaller}

Example of `Type_Amplitude` level feature, the Coincidence Factor:

```{.julia}
# Evaluate the coincidence factor between two sweeps (NB: the coincidence factor does not commute w.r.t. the order of the two sweeps)
function coincidence_factor_closure(δ::Int64)

    function coincidence_factor(experimental_peaks_t::Vector{Float64}, model_peaks_t::Vector{Float64} ) 

        interval = experimental_peaks_t[end] - experimental_peaks_t[1] + 2*δ

        # number of spiked of experimental trace
        N_d::Int64 = length(experimental_peaks_t)
        # number of spikes of model
        N_m::Int64 = length(model_peaks_t)
        # spike frequency model (in 1/ms, since it is multiplied by Δ ( = 4ms by default) in the return )
        f_m::Float64 = N_m / interval 

        # Count coincidences
        N_c::Int64 = 0 

        # details here: https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=675036bde791910851c7bf581ed7220e8f55ac53
        if !isempty(model_peaks_t)
            experimental_peaks_t_dc = deepcopy(experimental_peaks_t)
           
            for sol_t in model_peaks_t
                differences  = abs.(experimental_peaks_t_dc .-  sol_t)
                min_index = argmin(differences)
                if differences[min_index] <= δ
                    deleteat!(experimental_peaks_t_dc, min_index)
                    N_c += 1
                    if isempty(experimental_peaks_t_dc)
                        break
                    end
                end
            end
           
        end

        
        return ( (N_c - 2 * f_m * N_d * δ ) / (N_d + N_m) ) * ( 2 / ( 1 - 2 * f_m * δ )) 

    end
end


# coincidence_factor. used to implement Xu2019: https://ieeexplore.ieee.org/document/8682825
function coincidence_factor_loss_closure(δ::Int64)
    coincidence_factor = coincidence_factor_closure(δ)
    function coincidence_factor_loss(solution_peaks_t::Vector{Float64}, experimental_peaks_ts::Vector{Vector{Float64}}; intrinsic_reliability::Float64 ) # interval::Float64 

        # Mean the coincidence factors between the simulation and all the sweeps
        coincidence = mean([coincidence_factor(experimental_peaks_t, solution_peaks_t) for experimental_peaks_t in experimental_peaks_ts])

        # return 1 - the coincidence factor, so it is coherent with minimization (rather than maximization). Recall that the coincidence factor is at most 1. Negative values indicate that the simulated sweep reproduces spike times more consistently than the physical neuron intrinsic reliability, thus we may confidently return 0.  
        return maximum([1.0 - coincidence / intrinsic_reliability, 0.0])

    end
end

# Evaluation on electrophysiological recordings
coincidence_factor_evaluation_on_sweep(ephys_sfe::PyObject) = ephys_sfe.spike_feature("peak_t") .* 1e3

# Evaluation on model simulation
coincidence_factor_evaluation_on_sweep(sol::OrdinaryDiffEq.ODESolution) = sol.prob.p[1]

# Evaluate target and intrinsic variability
function coincidence_factor_target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude_closure(δ::Int64)

    coincidence_factor = coincidence_factor_closure(δ)

    function coincidence_factor_target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude(ephys_sfes::Vector{PyObject})

        experimental_peaks_ts = coincidence_factor_evaluation_on_sweep.(ephys_sfes) 
        experimental_coincidence_factors = [coincidence_factor(experimental_spike_times_1, experimental_spike_times_2) for (experimental_spike_times_1,experimental_spike_times_2) in Iterators.product(experimental_peaks_ts,experimental_peaks_ts)]

        intrinsic_reliability = mean(experimental_coincidence_factors)

        return experimental_peaks_ts, (intrinsic_reliability = intrinsic_reliability,), ()

    end
end

# Instantiate the features which depends on the interval δ (msec)
coincidence_factor_msec(δ::Int64 = 2) = ElectrophysiologicalFeature(
                            "coincidence_factor_msec",
                            1e-3,
                            coincidence_factor_evaluation_on_sweep,
                            coincidence_factor_target_lossKwargs_evaluationKwargs_evaluation_on_typeAmplitude_closure(δ), #ephys_sfe::PyObject -> ephys_sfe.spike_feature("peak_t") .* 1e3,
                            coincidence_factor_loss_closure(δ),
)
```


## Overview: Features #4 {.smaller .scrollable}

Other implemented features are:

- Well established:


    - allen_action_potential_peak_mV;
    - allen_fast_trough_depth_mV;
    - allen_slow_troughs_depth_mV;
    - allen_time_of_slow_troughs_fraction;
    - allen_AP_width_hh_msec;
    - allen_resting_potential_mV;
    - allen_latency_to_first_spike_msec;
    - allen_duration_of_first_isi_msec;
    - allen_isi_cv;
    - allen_adaptation_index;
    - allen_average_isi_msec;
    - allen_last_spike_latency_msec;
    - slope_after_stim_mV_msec;
    - custom_ISI_log_slope_msec;
    - spike_height_slope_mV;
    - slow_troughs_slope_mV;


- Experimental:


    - phase_plane_trajectory;
    - phase_plane_trajectory_druck;
    - custom_voltage_base_mV_druck;
    - custom_voltage_base_mV_l2;
    - custom_voltage_base_mV_mean;
    - custom_voltage_after_stim_mV_druck;
    - custom_voltage_after_stim_mV_l2;
    - custom_voltage_after_stim_mV_mean;
    - custom_steady_state_voltage_stimend_mV_druck;
    - custom_steady_state_voltage_stimend_mV_l2;
    - custom_steady_state_voltage_stimend_mV_mean;



## Overview: Associations #1 {.smaller}

- Different `Type_Amplitude`s may be calibrated w.r.t. different `Feature`s;

. . .

- E.g. it would make sense to calibrate the slope of the slow throughs w.r.t a ramp input, less so for a square input where such coefficient should be 0;

. . .

- Thus, the `Association` struct has been implemented to perform such assignments:

```{.julia}
struct Association <: AbstractData
    name::String
    data::Type_Amplitude
    features::Tuple{Vararg{ElectrophysiologicalFeature}}
    objectives_combinations::Dict{<:Tuple{Vararg{String}},<:Function}
end
```
. . .

- The `objectives_combinations` tells the inner functions how to combine losses associated sets of features of related semantics, as in [Hay 2011](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002107#s5) (supporting [Text S2](https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fstorage.googleapis.com%2Fplos-corpus-prod%2F10.1371%2Fjournal.pcbi.1002107%2F1%2Fpcbi.1002107.s010.doc%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dwombat-sa%2540plos-prod.iam.gserviceaccount.com%252F20230611%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20230611T174507Z%26X-Goog-Expires%3D86400%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7cfbbdf952e769f5c51c6fcd4f61c01081dd2b1113222e08ebeceb92596c6cda4158208df4100cfba21216dd2db8f5a1709305746b6d24f465dd176dfa69a4055d45ed4b4f2938876a2b434bfb925adf70f7dfbeecbe24d05f8e26630703880fe87a6c16a018d381d8edf7daf19c27ca284d90c4e96d94c5b5d91131c8e90d50955cc54255ef9b6b8437f1764b3f51f28dbf56d0388917477619c84942f6e69dcea9d4a336f86a3d239976864d8b61cca122d9f5c9468fe59c0610575ef447eab7266c7e61f5e2d6ed6507817570ebc8b3638f5e0e20a1bc2f8cfc429f9cbcacb6649c3a2a55707eba07fc2015a821d94cb7cef83d7249da9f74999ad9cb0f44&wdOrigin=BROWSELINK))


## Overview: Associations #2

Example `Association`s:

```{.julia}
const cell_dataset = get_cell_dataset(manifest_path,488697163)

# Square
const square_110pA           = cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"];

const square_features = (allen_firing_frequency_Hz(),allen_resting_potential_mV(), allen_latency_to_first_spike_msec(),allen_last_spike_latency_msec(),allen_average_isi_msec(), allen_slow_troughs_depth_mV(), coincidence_factor_msec(2))

const square_110_allen_association = Association("square_110_allen_association",square_110pA, square_features)

# Ramp
const ramp_800pA = get_type_amplitude(cell_dataset, "ramp", "800.0 pA")

const ramp_features = (allen_firing_frequency_Hz(),allen_resting_potential_mV(), allen_latency_to_first_spike_msec(),allen_last_spike_latency_msec(),allen_average_isi_msec(), allen_slow_troughs_depth_mV(), coincidence_factor_msec(2), slow_troughs_slope_mV())

const ramp_800_allen_association = Association("ramp_800_allen_association",ramp_800pA, ramp_features)
```


## Overview: Calibration Pattern #1 {.smaller}

- The model has `Parameter`s and `Initial_Condition`s. Since either of them may be tuned, they both subtype `Calibratable`:

```{.julia}
abstract type Calibratable{T,P} end
```
```{.julia}
mutable struct Parameter{T,P} <: AbstractParameter{T,P}
    name::String
    value::T
    prior::P
    calibratable::Bool
end
```
```{.julia}
mutable struct Initial_Condition{T,P} <: AbstractInitialCondition{T,P}
    name::String
    value::T
    prior::P
    calibratable::Bool
end
```

. . .

- All model `Parameter`s and `Initial_Condition`s form a `Configuration`:

```{.julia}
struct Configuration <: AbstractConfiguration
    initial_conditions::Union{ Nothing, Array{<:Initial_Condition} }
    parameters::Array{<:Parameter}
    model_posterior::Model_Posterior
end
# Where the `Model_Posterior` is the joint posterior over parameters and initial conditions
```

## Overview: Calibration Pattern #2 {.smaller}

- The optimization algorithm and/or method and calibration metadata are specified in the `Step` struct from `ModelingFramework.jl`:

```{.julia}
mutable struct Step <: AbstractStep
    name::String     # The name of the step. It is used as a key in the Calibration_History of the model
    method::Function # The step method. It must have signature (data::D, Model::M; kwargs...) where {D <: Tuple{Vararg{<:AbstractData}}, M <: AbstractModel}, and must return a valid Configuration.
    kwargs           # Step kwargs. It is a NamedTuple containing all extra named arguments to be passed to Step.method
end
```

. . .


- `Parameter`s and `Initial_Condition`s' priors are allowed to change between `Phase`s, which also serve as collection of `Step`s. For neuronal models, usually just one `Phase` is enough:

```{.julia}
mutable struct Phase{S <: Tuple{Vararg{<:AbstractStep}}, D <: Union{Nothing,Tuple{Vararg{<:AbstractData}}} } <: AbstractPhase
    name::String
    steps::S
    priors::Vector{<:Union{<: Nothing, <: Distribution{Univariate, Continuous}, <: BallTreeDensity} }
    start::Union{Nothing,Float64}
    finish::Union{Nothing,Float64}
    time_scale::Union{Nothing,Float64}
    data::D # For multiphase calibration
end
```

## Overview: Calibration Pattern #3 {.smaller}

An example of a `Step`'s method that uses the BORG-MOEA algorithm to perform multi-objective evolutionary calibration:

```{.julia}
function bbo_borgmoea_method(associations::Tuple{Vararg{Association}},
                             model::M; 
                             variable_parameters_names::Vector{String}, 
                             save_idxs::Vector{Int64}, 
                             modelingtoolkit::Bool = true, 
                             cell_dataset_optimization::Cell_Dataset_Optimization = Cell_Dataset_Optimization( trimming = false, mode = nothing), plot_optimized_sweeps::Bool = false, 
                             maxtime = 1, 
                             populationsize = 10* length(unique(combine_objectives_names(associations))),
                             verbose::Bool = false ) where {M<: Union{HH_model, IFModel}}

    # Group parameters into variable and fixed sets
    all_parameters_names =  get_last_parameters_names(model.calibration_history)
    variable_parameters =  select_calibratables(get_last_parameters(model.calibration_history),variable_parameters_names) 
    variable_parameters_indexes = [findfirst(all_sym -> all_sym == var_sym, all_parameters_names  ) for var_sym in variable_parameters_names ]
    fixed_parameters_indexes = setdiff(collect(1:length(all_parameters_names)), variable_parameters_indexes )
    fixed_parameters_values= get_calibratables_values( select_calibratables(get_last_parameters(model.calibration_history),setdiff(all_parameters_names,variable_parameters_names))  )
    variable_parameters_bounds::Vector{Tuple{Float64, Float64}} = [get_bounds_from_prior(prior, (0.0,1.0)) for prior in get_calibratables_priors(variable_parameters)]

    # Instantiate the loss. Note that in the "borg_moea" case, it is generated run time in order fo it to be type-stable;
    # `modelingtoolkit` controls symbolic rewriting of the deterministic_dynamics of the model to improve performance;
    # `cell_dataset_optimization` controls sweep truncation (and other deprecated behaviors) to improve performance;
    loss_neuro = neuron_optim_loss_closure(model, associations, variable_parameters_names,  save_idxs, "borg_moea"; modelingtoolkit = modelingtoolkit, cell_dataset_optimization  = cell_dataset_optimization, plot_optimized_sweeps =  plot_optimized_sweeps, verbose = verbose)

    # Launch multi-threaded calibration. The BORG-MOEA algorithm will auto-tune its hyperparameters, making it a suitable choice for black-box optimization
    dimensions = length(unique(combine_objectives_names(associations)))
    res = bboptimize(   loss_neuro; Method=:borg_moea, FitnessScheme=ParetoFitnessScheme{dimensions}(is_minimizing=true), SearchRange=variable_parameters_bounds, ϵ=0.05, TraceInterval=1.0, TraceMode=:verbose, MaxTime = maxtime, PopulationSize = populationsize, NThreads=Threads.nthreads()-1  );

    # Construct and return new Configuration
    all_calibrated_parameters_values = [couple[2] for couple in sort(collect(zip(vcat(fixed_parameters_indexes, variable_parameters_indexes), vcat(fixed_parameters_values, best_candidate(res)))); by=first)]
    all_calibrated_parameters        = change_values(get_last_parameters(model.calibration_history),all_calibrated_parameters_values)
    previous_model_posterior         = get_last_model_posterior(model.calibration_history)
    new_model_posterior              = Model_Posterior([Ordered_Multivariate(point_distribution(par.value), [par.name] ) for par in all_calibrated_parameters if par.calibratable ], previous_model_posterior.initial_conditions_size, previous_model_posterior.parameters_size  )
    new_configuration                = Configuration(get_last_initial_conditions(model.calibration_history),all_calibrated_parameters, new_model_posterior)

    return ((res = res,), new_configuration)

end
```

## Overview: Calibration Pattern #4{.smaller}

- So the main `Step` will be constructed as:

```{.julia}
borgmoea_step = Step( "borgmoea_step", bbo_borgmoea_method, kwargs...)
```

. . .

- We furthermore define some simpler `Step`s that will set a few parameters as they can be measured directly from the electrophysiological recordings:

```{.julia}
fix_urest_step = Step("fix_urest_step", fix_calibratable_method, (variable_parameter_name = "u_rest", feature = allen_resting_potential_mV()))
fix_V_th_step  = Step("fix_V_th_step",  fix_calibratable_method, (variable_parameter_name = "V_th",   feature = allen_action_potential_peak_mV()))
fix_u_r_step   = Step("fix_u_r_step",   fix_calibratable_method, (variable_parameter_name = "u_r",    feature = allen_fast_trough_depth_mV()))
```

. . .


- In this setting, calibration is initiated via:



```{.julia}
multi_step_calibration!((square_110_allen_association,), AdEx, (fix_urest_step, fix_V_th_step, fix_u_r_step, bbo_borgmoea_step))
```

## Overview: Calibration Pattern #5 {.smaller}

A `Step_Record` object si created every time a calibration `Step` is completed:
```{.julia}
struct Step_Record{T <: Union{Nothing,AbstractStep} } <: AbstractStepRecord
    step::T
    configuration::Configuration
    extras::NamedTuple
end
# The `configuration` field contains the new model Configuration after the `step` has been performed.
```
. . .

`Step_Record`s are then collected in `Phase_Record`s (usually neuronal models only have one phase and thus one `Phase_Record`):

```{.julia}
mutable struct Phase_Record{P <: Union{Nothing,AbstractPhase} } <: AbstractPhaseRecord
    phase::P
    steps_records::OrderedDict{<: String, <: AbstractStepRecord}
    extras::NamedTuple
end
```

. . .


Whose collection constitutes the model's `Calibration_History`:

```{.julia}
mutable struct Calibration_History <: AbstractCalibrationHistory
    phases_records::OrderedDict{<:String,<:Phase_Record}
end
```

That contains all `Configuration`s saved at every `Step` of every `Phase`.


# [NeuronalModelling.jl]{.semi-transparent}

1. [Why Julia?]{.semi-transparent}

2. [Overview;]{.semi-transparent}

3. Demonstration:

   3.1 First Example;

   3.2 Second Example.

4. [Future developments;]{.semi-transparent}


## Demonstration: First Example #1 {.smaller}

A complete example where we calibrate on `square_110pA` (for around 15 minutes) and test on `square_150pA`:

```{.julia}
using NeuronalModeling

const manifest_path = "path/to/allensdk/manifest"

const initial_parameters_values_AdEx = collect(values(NeuronalModeling.initial_parameters_values_AdEx_dict))
const parameters_priors_AdEx = get_uniform_priors(initial_parameters_values_AdEx, range = 1 )

const cell_dataset = get_cell_dataset(manifest_path,488697163)

# Plot Sweeps and Input corresponding to a Type_Amplitude
p_sweeps_square, p_input_square  = plot_type_amplitude(cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"]);

# AdEx model
AdEx = IFModel( "Adaptive Exponential",
                NeuronalModeling.AdEx_deterministic_dynamic!,
                NeuronalModeling.get_AdEx_IC,
                [name != "spike_times" ? Parameter(name, val, prior,true) : Parameter(name, val, prior,false) for (name, val, prior) in zip(NeuronalModeling.parameters_names_AdEx, initial_parameters_values_AdEx, parameters_priors_AdEx)],
                NeuronalModeling.AdEx_firing_discrete_callback 
);  


# Type_Amplitudes
const square_110pA           = cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"];
const square_150pA           = cell_dataset["data"]["square - 2s suprathreshold"]["150.0 pA"];

# Features
const square_simple_features = (allen_firing_frequency_Hz(),allen_resting_potential_mV(), allen_latency_to_first_spike_msec(),allen_last_spike_latency_msec(),allen_average_isi_msec(), allen_slow_troughs_depth_mV(), coincidence_factor_msec(2))

# Associations
const square_110_allen_association = Association("square_110_allen_association",square_110pA, square_simple_features)


# Define which parameters are fixed and which are variable
const variable_parameters_names = [par_name for par_name in NeuronalModeling.parameters_names_AdEx if par_name ∉ ["spike_times", "u_rest", "V_th", "u_r"] ]
const all_variable_parameters_names = [par_name for par_name in NeuronalModeling.parameters_names_AdEx if par_name ∉ ["spike_times"] ]

# Define multi-objective evolutionary step
const cell_dataset_optimization = Cell_Dataset_Optimization( true, nothing,  excluded_features = ("custom_voltage_base_mV_druck","custom_voltage_base_mV_l2","custom_steady_state_voltage_stimend_mV_druck", "custom_steady_state_voltage_stimend_mV_l2", "custom_voltage_after_stim_mV_druck", "custom_voltage_after_stim_mV_l2", "spike_times_msec"))
bbo_borgmoea_step = Step(   "bbo_borgmoea_step",
                            bbo_borgmoea_method, 
                            (variable_parameters_names = variable_parameters_names, save_idxs = [1], modelingtoolkit = true, cell_dataset_optimization = cell_dataset_optimization, maxtime = 15*60,  plot_optimized_sweeps = false, verbose = true ) ) 

# Define Steps that perform manual parameter identification
fix_urest_step = Step("fix_urest_step", fix_calibratable_method, (variable_parameter_name = "u_rest", feature = allen_resting_potential_mV()))
fix_V_th_step = Step("fix_V_th_step", fix_calibratable_method, (variable_parameter_name = "V_th", feature = allen_action_potential_peak_mV()))
fix_u_r_step = Step("fix_u_r_step", fix_calibratable_method, (variable_parameter_name = "u_r", feature = allen_fast_trough_depth_mV()))


# Perform the calibration
multi_step_calibration!((square_110_allen_association,), AdEx, (fix_urest_step, fix_V_th_step, fix_u_r_step, bbo_borgmoea_step))


# Plot results on training set
plot_integrate_and_fire_calibration(AdEx, Vector{Float64}(get_last_parameters_values(AdEx.calibration_history)[2:end]), square_110pA)

# Plot results on test set
plot_integrate_and_fire_calibration(AdEx, Vector{Float64}(get_last_parameters_values(AdEx.calibration_history)[2:end]), square_150pA)
```


## Demonstration: First Example #2 {.smaller}

Losses (in natural units) on train set **before** calibration:

```{.julia}
loss_square110pA = neuron_optim_loss_closure(AdEx, (square_110_allen_association,), all_variable_parameters_names,  [1], "borg_moea"; modelingtoolkit = true, cell_dataset_optimization  = cell_dataset_optimization, plot_optimized_sweeps =  false, verbose = true);

loss_square110pA(Vector{Float64}(initial_parameters_values_AdEx[2:end]))
```
```{.nothing}
"allen_latency_to_first_spike_msec" => 28.16
"allen_slow_troughs_depth_mV"       => 1.55801
"coincidence_factor_msec"           => 1.03656
"allen_last_spike_latency_msec"     => 0.536872
"allen_average_isi_msec"            => 4.06293
"allen_resting_potential_mV"        => 2.78894
"allen_firing_frequency_Hz"         => 1.1811
```

Losses (in natural units) on train set **after** calibration (analogous input code):
```{.nothing}
"allen_latency_to_first_spike_msec" => 1.00498
"allen_slow_troughs_depth_mV"       => 1.39227
"coincidence_factor_msec"           => 0.874528
"allen_last_spike_latency_msec"     => 0.59422
"allen_average_isi_msec"            => 0.798626
"allen_resting_potential_mV"        => 0.0140288
"allen_firing_frequency_Hz"         => 0.393699
```

So the aggregated loss in around 5, which far less than what Druckmann deems acceptable.


## Demonstration: First Example #3 {.smaller}

Model simulation **before** calibration superimposed to `square_110pA` sweeps:

![](images/plots/plot_calibration_AdEx_untrained_over_square_110pA.png){width=80% height=500} 


## Demonstration: First Example #4 {.smaller}

Model simulation **after** calibration superimposed to `square_110pA` sweeps:

![](images/plots/plot_calibration_AdEx_trained_square_110pA_over_square_110pA.png){width=80% height=500} 

## Demonstration: First Example #5 {.smaller}

Losses on test (square_150pA) **after** calibration:

```{.nothing}
"allen_latency_to_first_spike_msec" => 4.24117 
"allen_slow_troughs_depth_mV"       => 0.160079
"coincidence_factor_msec"           => 1.076   
"allen_last_spike_latency_msec"     => 1.72127 
"allen_average_isi_msec"            => 15.2966 
"allen_resting_potential_mV"        => 0.110215
"allen_firing_frequency_Hz"         => 8.66138
```

Which, if aggregated, returns around $31$, that is greater than $2 \times \text{num_features} = 2 \times 7 = 14$.

. . .

$\rightarrow$ We may improve this result by training on a wider dynamic range that extends below and above the `square_150pA`. For example, we could train on `square_110pA` and on `square_190pA`.

## Demonstration: Second Example #1 {.smaller}

A complete example where we calibrate over `square_110pA` and `square_190pA` (for around 1 hour), test over `square_150pA`:

```{.julia}
using NeuronalModeling

const manifest_path = "path/to/allensdk/manifest"

const initial_parameters_values_AdEx = collect(values(NeuronalModeling.initial_parameters_values_AdEx_dict))
const parameters_priors_AdEx = get_uniform_priors(initial_parameters_values_AdEx, range = 1 )

const cell_dataset = get_cell_dataset(manifest_path,488697163)

# Plot Sweeps and Input corresponding to a Type_Amplitude
p_sweeps_square, p_input_square  = plot_type_amplitude(cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"]);

# AdEx model
AdEx = IFModel( "Adaptive Exponential",
                NeuronalModeling.AdEx_deterministic_dynamic!,
                NeuronalModeling.get_AdEx_IC,
                [name != "spike_times" ? Parameter(name, val, prior,true) : Parameter(name, val, prior,false) for (name, val, prior) in zip(NeuronalModeling.parameters_names_AdEx, initial_parameters_values_AdEx, parameters_priors_AdEx)],
                NeuronalModeling.AdEx_firing_discrete_callback 
);  

# Type_Amplitudes
const square_110pA           = cell_dataset["data"]["square - 2s suprathreshold"]["110.0 pA"];
const square_150pA           = cell_dataset["data"]["square - 2s suprathreshold"]["150.0 pA"];
const square_190pA           = cell_dataset["data"]["square - 2s suprathreshold"]["150.0 pA"];

# Features
const square_simple_features = (allen_firing_frequency_Hz(),allen_resting_potential_mV(), allen_latency_to_first_spike_msec(),allen_last_spike_latency_msec(),allen_average_isi_msec(), allen_slow_troughs_depth_mV(), coincidence_factor_msec(2))

# Associations
const square_110_allen_association = Association("square_110_allen_association",square_110pA, square_simple_features)
const square_190_allen_association = Association("square_190_allen_association",square_110pA, square_simple_features)

# Define which parameters are fixed and which are variable
const variable_parameters_names = [par_name for par_name in NeuronalModeling.parameters_names_AdEx if par_name ∉ ["spike_times", "u_rest", "V_th", "u_r"] ]
const all_variable_parameters_names = [par_name for par_name in NeuronalModeling.parameters_names_AdEx if par_name ∉ ["spike_times"] ]

# Define multi-objective evolutionary step
const cell_dataset_optimization = Cell_Dataset_Optimization( true, nothing,  excluded_features = ("custom_voltage_base_mV_druck","custom_voltage_base_mV_l2","custom_steady_state_voltage_stimend_mV_druck", "custom_steady_state_voltage_stimend_mV_l2", "custom_voltage_after_stim_mV_druck", "custom_voltage_after_stim_mV_l2", "spike_times_msec"))
bbo_borgmoea_step = Step(   "bbo_borgmoea_step",
                            bbo_borgmoea_method, 
                            (variable_parameters_names = variable_parameters_names, save_idxs = [1], modelingtoolkit = true, cell_dataset_optimization = cell_dataset_optimization, maxtime = 60*60,  plot_optimized_sweeps = false, verbose = true ) ) 

# Define Steps that perform manual parameter identification
fix_urest_step = Step("fix_urest_step", fix_calibratable_method, (variable_parameter_name = "u_rest", feature = allen_resting_potential_mV()))
fix_V_th_step = Step("fix_V_th_step", fix_calibratable_method, (variable_parameter_name = "V_th", feature = allen_action_potential_peak_mV()))
fix_u_r_step = Step("fix_u_r_step", fix_calibratable_method, (variable_parameter_name = "u_r", feature = allen_fast_trough_depth_mV()))


# Perform the calibration
multi_step_calibration!((square_110_allen_association,square_110_allen_association), AdEx, (fix_urest_step, fix_V_th_step, fix_u_r_step, bbo_borgmoea_step))


# Plot results on training set
plot_integrate_and_fire_calibration(AdEx, Vector{Float64}(get_last_parameters_values(AdEx.calibration_history)[2:end]), square_110pA)
plot_integrate_and_fire_calibration(AdEx, Vector{Float64}(get_last_parameters_values(AdEx.calibration_history)[2:end]), square_190pA)

# Plot results on test set
plot_integrate_and_fire_calibration(AdEx, Vector{Float64}(get_last_parameters_values(AdEx.calibration_history)[2:end]), square_150pA)
```

## Demonstration: Second Example #2 {.smaller}

Losses (in natural units) on train set `square_110pA` **after** calibration:
```{.nothing}
"allen_latency_to_first_spike_msec_square__2s_suprathreshold_110_0_pA" => 5.14464
"allen_slow_troughs_depth_mV_square__2s_suprathreshold_110_0_pA"       => 3.71556
"coincidence_factor_msec_square__2s_suprathreshold_110_0_pA"           => 1.0252
"allen_last_spike_latency_msec_square__2s_suprathreshold_110_0_pA"     => 0.518066
"allen_average_isi_msec_square__2s_suprathreshold_110_0_pA"            => 0.439908
"allen_resting_potential_mV_square__2s_suprathreshold_110_0_pA"        => 0.0162544
"allen_firing_frequency_Hz_square__2s_suprathreshold_110_0_pA"         => 0.393699
```

We have two losses above 2 SD $\rightarrow$ we pay something.

## Demonstration: Second Example #3 {.smaller}

Losses on train set `square_190pA` **before** calibration:
```{.nothing}
"allen_latency_to_first_spike_msec" => 20.3146
"allen_slow_troughs_depth_mV"       => 2.75719
"coincidence_factor_msec"           => 0.994487
"allen_last_spike_latency_msec"     => 0.306099
"allen_average_isi_msec"            => 25.8072
"allen_resting_potential_mV"        => 2.80781
"allen_firing_frequency_Hz"         => 13.435
```
Whose aggregation is around 66.

Losses on train set `square_190pA` **after** calibration:
```{.nothing}
"allen_latency_to_first_spike_msec_square__2s_suprathreshold_190_0_pA" => 2.17963
"allen_slow_troughs_depth_mV_square__2s_suprathreshold_190_0_pA"       => 0.0694215
"coincidence_factor_msec_square__2s_suprathreshold_190_0_pA"           => 0.980084
"allen_last_spike_latency_msec_square__2s_suprathreshold_190_0_pA"     => 0.140696
"allen_average_isi_msec_square__2s_suprathreshold_190_0_pA"            => 0.696329
"allen_resting_potential_mV_square__2s_suprathreshold_190_0_pA"        => 0.0588253
"allen_firing_frequency_Hz_square__2s_suprathreshold_190_0_pA"         => 0.707107
```
Only one loss slightly above 2 SD $\rightarrow$ very good!

## Demonstration: Second Example #4 {.smaller}

Losses on test set `square_150pA` **after** calibration:
```{.nothing}
"allen_latency_to_first_spike_msec_square__2s_suprathreshold_150_0_pA" => 3.17513
"allen_slow_troughs_depth_mV_square__2s_suprathreshold_150_0_pA"       => 1.2069
"coincidence_factor_msec_square__2s_suprathreshold_150_0_pA"           => 1.15644
"allen_last_spike_latency_msec_square__2s_suprathreshold_150_0_pA"     => 0.520282
"allen_average_isi_msec_square__2s_suprathreshold_150_0_pA"            => 4.2622
"allen_resting_potential_mV_square__2s_suprathreshold_150_0_pA"        => 0.0998484
"allen_firing_frequency_Hz_square__2s_suprathreshold_150_0_pA"         => 3.14959
```
Whose aggregation is around 13.5 $\rightarrow$ much less than before (31) $\rightarrow$ better generalization!

## Demonstration: Second Example #5 {.smaller}

![](images/plots/plot_calibration_AdEx_trained_square_110pA_square_190pA_over_square_110pA.png){width=80% height=500} 

## Demonstration: Second Example #6 {.smaller}

![](images/plots/plot_calibration_AdEx_trained_square_110pA_square_190pA_over_square_190pA.png){width=80% height=500} 

## Demonstration: Second Example #7 {.smaller}

![](images/plots/plot_calibration_AdEx_trained_square_110pA_square_190pA_over_square_150pA.png){width=80% height=500}


# [NeuronalModelling.jl]{.semi-transparent}


1. [Why Julia?]{.semi-transparent}

2. [Overview;]{.semi-transparent}

3. [Demonstration;]{.semi-transparent}

4. Future developments:

    4.1 Hodgkin-Huxley models;

    4.2 Macros;

    4.3 Other improvements;


## Future developments: Hodgkin-Huxley models {.smaller .scrollable}

- Earlier versions of the package were centered around Hodgkin-Huxley models. Although we managed to calibrate them, they presented much harder challenges;

. . .

- We therefore switched in favor of AdNLIF models. Now that we perfected the pipeline for the simpler AdNLIF case, we may feel more confident to walk the HH path again.

. . .

```{.julia}
# function that outputs initial conditions for fractions of open channels, given the value of the resting potential.
## The idea behind this function has been drawn from http://indico.ictp.it/event/a13235/session/99/contribution/371/material/0/1.pdf
## Note: it MUST take all parameters values as argument, for later ease of use in the neuron_optim_loss_closure
function get_HH__Na_Kd_M_L_mf_exp_IC(V_rest::Float64, all_parameters_values::Array{Float64,1}, type_amplitude::String)
    C,     g_Na,  V_Na,        g_K,   g_M,   V_K,   g_L,   V_Ca,  g_l, V_l, V_T,
    m_α_A, m_α_C,       m_β_A, m_β_C,
    h_α_A, h_α_C,       h_β_A, h_β_C,
    n_α_A, n_α_C,       n_β_A, n_β_C,
    p_0_B, p_0_C, τ_p_max_exp, τ_p_A, τ_p_B, τ_p_C,
    q_α_A, q_α_B, q_α_C,       q_β_A, q_β_B, q_β_C,
    r_α_A, r_α_B, r_α_C,       r_β_A, r_β_B, r_β_C    = all_parameters_values

    αₘ = (- m_α_A * ( V_rest - V_T - 13.0 ) / (exp( - ( V_rest - V_T - 13.0 ) / m_α_C) ) -1.0 )
    βₘ = (m_β_A * ( V_rest - V_T - 40.0 ) / ( exp( ( V_rest - V_T - 40.0 ) / m_β_C) ) - 1.0 )

    αₕ =  (h_α_A * exp( - ( V_rest - V_T - 17.0 ) / h_α_C ) )
    βₕ =  (h_β_A  /( 1.0 + exp( - ( V_rest - V_T - 40.0  ) / h_β_C )) )

    αₙ = (- n_α_A * ( V_rest - V_T - 15.0 ) / (exp( - ( V_rest - V_T - 15.0  ) / n_α_C) ) - 1.0) 
    βₙ = (n_β_A * exp( - ( V_rest - V_T - 10.0 ) / n_β_C) )

    x₀ = ( (1.0 / ( 1.0 + exp( - ( V_rest + p_0_B )/p_0_C ) ) ) )

    α_4 =  ( ( q_α_A * exp(- V_rest - q_α_B ) ) /(exp((- V_rest - q_α_B )/q_α_C ) - 1.0) )
    β_4 = ( q_β_A * exp((- V_rest - q_β_B ) / q_β_C) )

    αᵣ = ( r_α_A * exp((- V_rest - r_α_B ) / r_α_C) )
    βᵣ = ( r_β_A   /(exp((- V_rest - r_β_B )/r_β_C ) + 1.0) )

    m₀ = αₘ / ( αₘ + βₘ )
    h₀ = αₕ / ( αₕ + βₕ  )
    n₀ = αₙ / ( αₙ + βₙ  )
    p₀ = x₀_p
    q₀ = α_q / (α_q + β_q)
    r₀ = αᵣ / (αᵣ + βᵣ )

    return vcat(V_rest, m₀, h₀, n₀, p₀, q₀, r₀)
end

function get_HH__Na_Kd_M_L_mf_tanh_IC(V_rest::Float64, all_parameters_values::Vector{Float64}, type_amplitude = "")
    C,   g_Na, V_Na, g_K,     g_M,   V_K,   g_L, V_Ca, g_l, V_l,
    V_m, κ_m,  τ₀_m, τ_max_m, V_τ_m, σ_m, 
    V_h, κ_h,  τ₀_h, τ_max_h, V_τ_h, σ_h,
    V_n, κ_n,  τ₀_n, τ_max_n, V_τ_n, σ_n,
    V_p, κ_p,  τ₀_p, τ_max_p, V_τ_p, σ_p,
    V_q, κ_q,  τ₀_q, τ_max_q, V_τ_q, σ_q,
    V_r, κ_r,  τ₀_r, τ_max_r, V_τ_r, σ_r     = all_parameters_values

    m₀ = (1/2) * (1 + tanh( (V_rest - V_m) / κ_m ) )
    h₀ = (1/2) * (1 + tanh( (V_rest - V_h) / κ_h ) )
    n₀ = (1/2) * (1 + tanh( (V_rest - V_n) / κ_n ) )
    p₀ = (1/2) * (1 + tanh( (V_rest - V_p) / κ_p ) )
    q₀ = (1/2) * (1 + tanh( (V_rest - V_q) / κ_q ) )
    r₀ = (1/2) * (1 + tanh( (V_rest - V_r) / κ_r ) )

    return vcat(V_rest, m₀, h₀, n₀, p₀, q₀, r₀ )
end



function HH__Na_Kd_M_L_mf_exp_deterministic_dynamic!(I_series::F) where {F <:Function}
    function parametrized_HH__Na_Kd_M_L_mf_exp_deterministic_dynamic!(du, u, p, t )
        
        C,     g_Na,  V_Na,        g_K,   g_M,   V_K,   g_L,   V_Ca,  g_l, V_l, V_T,
        m_α_A, m_α_C,       m_β_A, m_β_C,
        h_α_A, h_α_C,       h_β_A, h_β_C,
        n_α_A, n_α_C,       n_β_A, n_β_C,
        p_0_B, p_0_C, τ_p_max_exp, τ_p_A, τ_p_B, τ_p_C,
        q_α_A, q_α_B, q_α_C,       q_β_A, q_β_B, q_β_C,
        r_α_A, r_α_B, r_α_C,       r_β_A, r_β_B, r_β_C    = p 
        
        # State variables
        V = @view u[1]
        m = @view u[2]
        h = @view u[3] 
        n = @view u[4] 
        p = @view u[5]
        q = @view u[6]
        r = @view u[7]

        # State variables
        dV = @view du[1]
        dm = @view du[2]
        dh = @view du[3] 
        dn = @view du[4] 
        dp = @view du[5]
        dq = @view du[6]
        dr = @view du[7]


        dV .= (-1.0 / C ) .* ( g_Na .* (m .^ 3) .* h .* ( V .- V_Na ) .+ g_K .* (n .^ 4 ) .* ( V .- V_K ) .+ g_M .* p .* (V - V_K) .+ g_L .* (q.^2)*r*(V .- V_Ca ) .+ g_l .* ( V .- V_l ) .- I_series(t)   ) 

        @. dm =  (- m_α_A * ( V - V_T - 13.0 ) / (exp( - ( V - V_T - 13.0 ) / m_α_C) ) -1.0 )  * ( 1 - m ) -  (m_β_A * ( V - V_T - 40.0 ) / ( exp( ( V - V_T - 40.0 ) / m_β_C) ) - 1.0 )  * m   

        @. dh =  (h_α_A * exp( - ( V - V_T - 17.0 ) / h_α_C ) )  * ( 1 - h ) - (h_β_A  /( 1.0 + exp( - ( V - V_T - 40.0  ) / h_β_C )) ) * h  

        @. dn =  (- n_α_A * ( V - V_T - 15.0 ) / (exp( - ( V - V_T - 15.0  ) / n_α_C) ) - 1.0) * ( 1 - n ) - (n_β_A * exp( - ( V - V_T - 10.0 ) / n_β_C) ) * n 

        @. dp = ( (1. / ( 1 + exp( - ( V + p_0_B )/p_0_C ) ) ) - p) /  (τ_p_max_exp / (τ_p_A * exp( (V + τ_p_B)/τ_p_C ) + exp(- ( V + τ_p_B )/τ_p_C ) ) )

        @. dq = ( ( q_α_A * exp(- V - q_α_B ) ) /(exp((- V - q_α_B )/q_α_C ) - 1.0) ) * (1-q) - ( q_β_A * exp((- V - q_β_B ) / q_β_C) ) * q

        @. dr = ( r_α_A * exp((- V - r_α_B ) / r_α_C) ) * (1-r) - ( r_β_A   /(exp((- V - r_β_B )/r_β_C ) + 1.0) ) * r

    end
end


function HH__Na_Kd_M_L_mf_tanh_deterministic_dynamic!(I_series::F) where {F <:Function}
    function parametrized_HH__Na_Kd_M_L_tanh_deterministic_dynamic!(du, u, p, t)

        C,   g_Na, V_Na, g_K,     g_M,   V_K,   g_L, V_Ca, g_l, V_l,
        V_m, κ_m,  τ₀_m, τ_max_m, V_τ_m, σ_m, 
        V_h, κ_h,  τ₀_h, τ_max_h, V_τ_h, σ_h,
        V_n, κ_n,  τ₀_n, τ_max_n, V_τ_n, σ_n,
        V_p, κ_p,  τ₀_p, τ_max_p, V_τ_p, σ_p,
        V_q, κ_q,  τ₀_q, τ_max_q, V_τ_q, σ_q,
        V_r, κ_r,  τ₀_r, τ_max_r, V_τ_r, σ_r        = p

        # State variables
        V = @view u[1]
        m = @view u[2]
        h = @view u[3] 
        n = @view u[4] 
        p = @view u[5]
        q = @view u[6]
        r = @view u[7]

        # State variables
        dV = @view du[1]
        dm = @view du[2]
        dh = @view du[3] 
        dn = @view du[4] 
        dp = @view du[5]
        dq = @view du[6]
        dr = @view du[7]

        dV .= (.-1.0 ./ C ) .* ( g_Na .* (m .^ 3) .* h .* ( V .- V_Na ) .+ g_K .* (n .^ 4 ) .* ( V .- V_K ) .+ g_M .* p .* (V .- V_K) .+ g_L .* (q.^2)*r*(V .- V_Ca ) .+ g_l .* ( V .- V_l ) .- I_series(t)   ) 

        @. dm = ( (1/2) * (1 + tanh( (V - V_m) / κ_m ) ) - m ) / ( τ₀_m + τ_max_m * ( 1 - tanh( ( V - V_τ_m  ) / σ_m ) ^ 2 ) )  

        @. dh = ( (1/2) * (1 + tanh( (V - V_h) / κ_h ) ) - h ) / ( τ₀_h + τ_max_h * ( 1 - tanh( ( V - V_τ_h  ) / σ_h ) ^ 2 ) )

        @. dn = ( (1/2) * (1 + tanh( (V - V_n) / κ_n ) ) - n ) / ( τ₀_n + τ_max_n * ( 1 - tanh( ( V - V_τ_n  ) / σ_n ) ^ 2 ) ) 

        @. dp = ( (1/2) * (1 + tanh( (V - V_p) / κ_p ) ) - p ) / ( τ₀_p + τ_max_p * ( 1 - tanh( ( V - V_τ_p  ) / σ_p ) ^ 2 ) ) 

        @. dq = ( (1/2) * (1 + tanh( (V - V_q) / κ_q ) ) - q ) / ( τ₀_q + τ_max_q * ( 1 - tanh( ( V - V_τ_q  ) / σ_q ) ^ 2 ) ) 

        @. dr = ( (1/2) * (1 + tanh( (V - V_r) / κ_r ) ) - r ) / ( τ₀_r + τ_max_r * ( 1 - tanh( ( V - V_τ_r  ) / σ_r ) ^ 2 ) ) 

    end
end

const initial_parameters_values_HH__Na_Kd_M_L_mf_dict=OrderedDict( "C"     => 1.0   , "g_Na"  => 50.0, "V_Na"  => 50.0, "g_K" => 5.0, "g_M "=> 0.004 , "V_K" => -90.0, "g_L" => 0.1, "V_Ca" => 120.0,  "g_l" => 0.01, "V_l" => -70.61,  "V_T" => -50.0 ,

                                                                "m_α_A"       => 0.32    , "m_α_C" => 4.0 ,                                  # αₘ
                                                                "m_β_A"       => 0.28    , "m_β_C" => 5.0 ,                                  # βₘ
                                                                "h_α_A"       => 0.128   , "h_α_C" => 18.0,                                  # αₕ
                                                                "h_β_A"       => 4.0     , "h_β_C" => 5.0 ,                                  # βₕ
                                                                "n_α_A"       => 0.032   , "n_α_C" => 5.0 ,                                  # αₙ
                                                                "n_β_A"       => 0.5     , "n_β_C" => 40.0,                                  # βₙ
                                                                "p_0_B"       => 35.0    , "p_0_C" => 10.0,                                  # p_x₀
                                                                "τ_p_max_exp" => 4000.0  , "τ_p_A" => 3.3 , "τ_p_B" => 35.0, "τ_p_C"=> 20.0,  # p_τ
                                                                "q_α_A"       => 0.055   , "q_α_B" => 27.0, "q_α_C" => 3.8 ,                  #α_q
                                                                "q_β_A"       => 0.94    , "q_β_B" => 75.0, "q_β_C" => 17.0,                  # β_q
                                                                "r_α_A"       => 0.000457, "r_α_B" => 13.0, "r_α_C" => 50.0,                  # α_r
                                                                "r_β_A"       => 0.0065  , "r_β_B" => 15.0, "r_β_C" => 28.0                   # β_r
)
```

::: {.notes}
- The exp version is more common in literature and easier to implement;

- The tanh version yields higher numerical performances;

- We also implemented a mechanism that would allow to switch between the two.
:::

## Future developments: Macros #1 {.smaller}

- One of Julia's most advanced features is *metaprogramming*:

```{.julia}
for op = (:sin, :cos, :tan, :log, :exp)
    @eval Base.$op(a::MyNumber) = MyNumber($op(a.x))
end
```
. . .

- `@macro`s are special functions that allow code generation at run-time. Here is an example from NeuronalModeling.jl:

```{.julia}
 loss_expr = :( 
            
        function borg_moea_loss(theta::Vector{Float64})::NTuple{$num_features,Float64}

            losses_values_by_feature_dict = hh_neuron_loss_intermediate($hh_model, theta, $fixed_parameters_indexes, $variable_parameters_indexes, deepcopy($fixed_parameters_values), $save_idxs, $filter, $resting_potentials_values_by_type_amplitude, $features_targets_lossKwargs_by_type_amplitude, $opt_trim_associations, $fast_problems_by_type_amplitude_dict, $time_steps_by_type_amplitude_dict, $tstops_by_type_amplitude_dict, $features_counts_dict, $electrophysiology_backend, $aggregation_strategy_of_same_feature_across_different_type_amplitudes)

            return NTuple{$num_features,Float64}([loss_val <= 1000.0 ? loss_val : 1000.0 for loss_val in values(losses_values_by_feature_dict)]) # length(losses_values_by_feature_dict)
        end
    )

    end


    return @RuntimeGeneratedFunction(loss_expr)
```

## Future developments: Macros #2 {.smaller}

- From the code of the AdEx and the HH models, it should be evident that they are quite hard and error prone to code by hand;

. . .

- Due to standard patterns occurring during the definition of both AdNLIF and HH models, we may implement macros to greatly ease their instantiation;

. . .

- They would look something like:

```{.julia}
# AdNLIF
@AdNLIF [nonlinearity] [n° auxiliary variables]

# HH

@HH [currents]
```

. . .

- Should be easy for `@AdNLIF`;

## Future developments: Macros #3 {.smaller}

- For `@HH`, we may adopt packages like [`Conductor.jl`](https://github.com/wsphillips/Conductor.jl), or make us of our already-existing implementation of `Current`s:

```{.julia}
mutable struct Gating_Variable_Boltzmann_Term{F<:Function}
    names::Vector{String}
    evaluation::F
end

# struct that represents a gating variable rate
mutable struct Gating_Variable_Rate{F<:Function}
    names::Vector{String}
    evaluation::F
end

# struct that represents a gating variable
# https://github.com/mauro3/Parameters.jl and https://discourse.julialang.org/t/keyword-argument-constructor-breaks-incomplete-constructor/34198
@with_kw mutable struct Gating_Variable
    name::String
    x₀::Gating_Variable_Boltzmann_Term
    τ::Gating_Variable_Boltzmann_Term
end

#keyword constructor using rates
function Gating_Variable(name::String; α::Gating_Variable_Rate, β::Gating_Variable_Rate)
    names::Vector{String} = vcat(α.names, β.names)
    x₀_evaluation = function (V::Float64, p::Vector{Float64})
        return α.evaluation(V, p[1:length(α.names)]) / (α.evaluation(V, p[1:length(α.names)]) + β.evaluation(V, p[length(α.names)+1:end]))
    end
    x₀::Gating_Variable_Boltzmann_Term = Gating_Variable_Boltzmann_Term(names, x₀_evaluation)
    τ_evaluation = function (V::Float64, p::Vector{Float64})
        return 1 / (α.evaluation(V, p[1:length(α.names)]) + β.evaluation(V, p[length(α.names)+1:end]))
    end
    τ::Gating_Variable_Boltzmann_Term = Gating_Variable_Boltzmann_Term(names, τ_evaluation)
    return Gating_Variable(name=name, x₀=x₀, τ=τ)

end

# struct that represents a current
# https://docs.julialang.org/en/v1/manual/faq/#faq-nothing
mutable struct Current
    activating::Union{Gating_Variable,Nothing}
    deactivating::Union{Gating_Variable,Nothing}
    gating_variables_names::Vector{String}
    activating_exponent::Float64
    deactivating_exponent::Float64
    all_parameter_names::Union{Vector{String},Nothing}
    gating_variables_indexes::Dict{String,Dict{String,Vector{Int64}}}
end
```

## Future developments: Other improvements

- [Initial condition calibration](https://github.com/InPhyT/NeuronalModeling.jl/issues/33) $\rightarrow$ attempt 2009 challenge!

. . .

- [Switch from AllenSDK to IPFX](https://github.com/InPhyT/NeuronalModeling.jl/issues/36);

. . .

- Statistical calibration, Kalman Filters, Simulation Based Inference (see [#38](https://github.com/InPhyT/NeuronalModeling.jl/issues/38), [#35](https://github.com/InPhyT/NeuronalModeling.jl/issues/35) and [#34](https://github.com/InPhyT/NeuronalModeling.jl/issues/34));

. . .

- [Automate handling of units of measure with `Unitful.jl`](https://github.com/InPhyT/NeuronalModeling.jl/issues/29);

. . .

- *Etc.*

# :package: Related Packages 

## Related Packages 

:::: {.columns}

::: {.column width="50%"}
### Julia

- [**BlackBoxOptim**](https://github.com/robertfeldt/BlackBoxOptim.jl)
- [**DifferentialEquations**](https://github.com/SciML/DifferentialEquations.jl)
- [**Evolutionary**](https://github.com/wildart/Evolutionary.jl)
- [**Metaheuristics**](https://github.com/jmejia8/Metaheuristics.jl)
- [**ModelingToolkit**](https://github.com/SciML/ModelingToolkit.jl)
- [**Optim**](https://github.com/JuliaNLSolvers/Optim.jl)
- [Turing](https://github.com/TuringLang/Turing.jl)
- [Conductor](https://github.com/wsphillips/Conductor.jl)

![](images/logo/JuliaDots.svg){.absolute top=83 left=130 width=5%}
:::

::: {.column width="50%"}
### Python 

- [**AllenSDK**](https://github.com/AllenInstitute/AllenSDK)
- [BluePyEfe](https://github.com/BlueBrain/BluePyEfe)
- [BluePyOpt](https://github.com/BlueBrain/BluePyOpt)
- [**eFEL**](https://github.com/BlueBrain/eFEL)
- [NEURON](https://github.com/neuronsimulator/nrn)
- [Optimizer](https://github.com/KaliLab/optimizer)
- [PyNEST](https://nest-simulator.readthedocs.io/en/stable/tutorials/index.html)

![](images/logo/Python.png){.absolute top=83 right=260 width=5%}
:::

::::

# :books: References

## References {.smaller}

:::: {.columns}

::: {.column width="35%"}
### Present 

- [Brette and Gerstner (2005)](https://doi.org/10.1152/jn.00686.2005)
- [Pospischil et al. (2008)](https://doi.org/10.1007/s00422-008-0263-8)
- [Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)
- [Druckmann et al. (2008)](https://doi.org/10.1007/s00422-008-0269-2) 
- [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x)
- [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936)
- [Druckmann et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002133) 
- [Toth et al. (2011)](https://doi.org/10.1007/s00422-011-0459-1)
- [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en)
- [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28)
- [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615)
:::

::: {.column width="35%"}

### Future 

- [Van Geit et al. (2008)](https://doi.org/10.1007/s00422-008-0257-6) 
- [Kobayashi et al. (2009)](https://doi.org/10.3389/neuro.10.009.2009)
- [Smolinski et al. (2009)](https://doi.org/10.1186/1471-2202-10-s1-p260)
- [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208)
- [Svensson et al. (2012)](https://doi.org/10.1007/s12021-012-9140-7)
- [Friedrich et al. (2014)](https://doi.org/10.3389/fninf.2014.00063)
- [Druckmann (2014)](https://doi.org/10.1007/978-1-4614-7320-6_159-1)
- [Meliza et al. (2014)](https://doi.org/10.1007/s00422-014-0615-5) 
- [Lynch and Houghton (2015)](https://doi.org/10.3389/fninf.2015.00010)
- [Van Geit et al. (2016)](https://doi.org/10.3389/fninf.2016.00017)
- [Masoli et al. (2017)](https://doi.org/10.3389/fncel.2017.00071)
:::

::: {.column width="30%"}

### Future 

- [Gonçalves et al. (2020)](https://doi.org/10.7554/elife.56261)
- [Mohacsi et al. (2020)](https://doi.org/10.1109/ijcnn48605.2020.9206692)
- [Saray et al. (2021)](https://doi.org/10.1371/journal.pcbi.1008114)
- [Gala et al. (2021)](https://doi.org/10.1038/s43588-021-00030-1)
- [AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)
- [Kreuz et al. (2022)](https://doi.org/10.1007/978-1-0716-1006-0_409)
- [Nandi et al. (2022)](https://doi.org/10.1016/j.celrep.2022.111176)
:::
::::

::: {.notes}
...
:::

# Thanks :pray:

---

## {.smaller}

### Outline 

:one: [Objectives](#Objectives)

:two: [Motivations](#Motivations)

:three: [Methods](#Methods)

:four: [Tools](#Tools)

:five: [Results](#Results)

### Resources 

![](images/logo/JuliaDots.svg){.absolute bottom=252 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [ModellingFramework.jl](https://github.com/InPhyT/ModellingFramework.jl) 

![](images/logo/JuliaDots.svg){.absolute bottom=205 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [NeuronalModelling.jl](https://github.com/InPhyT/NeuronalModelling.jl) 

![](images/logo/GitHub.svg){.absolute bottom=155 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2007)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2007) 

![](images/logo/GitHub.svg){.absolute bottom=107 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2009)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2009) 

![](images/logo/GitHub.svg){.absolute bottom=59 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Allen Brain Atlas (2023)](https://github.com/AllenInstitute/AllenSDK) 

![](images/logo/NeuronalModelling.svg){.absolute bottom=120 right=0 width=45%}

::: {.notes}
...
:::