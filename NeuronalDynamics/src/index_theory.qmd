---
title: "NeuronalModelling.jl"
subtitle: "Quantitative Single-Neuron Modelling in Julia"
---

# :clapper: Introduction 

::: {.notes}
...
:::

---

## {.smaller}

### Outline 

:one: [Objectives](#Objectives)

:two: [Motivations](#Motivations)

:three: [Methods](#Methods)

:four: [Tools](#Tools)

:five: [Results](#Results)

### Resources 

![](images/logo/JuliaDots.svg){.absolute bottom=252 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [ModellingFramework.jl](https://github.com/InPhyT/ModellingFramework.jl) 

![](images/logo/JuliaDots.svg){.absolute bottom=205 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [NeuronalModelling.jl](https://github.com/InPhyT/NeuronalModelling.jl) 

![](images/logo/GitHub.svg){.absolute bottom=155 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2007)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2007) 

![](images/logo/GitHub.svg){.absolute bottom=107 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2009)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2009) 

![](images/logo/GitHub.svg){.absolute bottom=59 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Allen Brain Atlas (2023)](https://github.com/AllenInstitute/AllenSDK) 

![](images/logo/NeuronalModelling.svg){.absolute bottom=120 right=0 width=45%}

::: {.notes}
...
:::

<!-- https://github.com/quarto-dev/quarto-cli/discussions/4572 -->
```{=html}
<style>
.semi-transparent {
  opacity: 0.3;
}
</style>
```

# :exclamation: Objectives

## Objectives {.smaller}

:::: {.columns}

::: {.column width="40%"}
### Theory 

Flexible and high-performance computational framework for the specification, calibration and simulation of single-neuron models.

### Application

Electrophysiological time series inference and prediction for individual neurons.
:::

::: {.column width="60%"}
![](images/articles/2009Gerstner.png){.absolute bottom=90 right=-180 width=80%}
:::

::::

::: {.aside}
Figure from [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936).
:::

::: {.notes}
GENERAL / THEORY / METHODOLOGY 

Construction / Development / Implementation of an automatic, high-performance, quantitative single-neuron modelling framework providing a robust specification-calibration-simulation workflow. 

SPECIFIC / APPLICATION / DATA

Single-neuron electrophysiological time series inference and prediction.

REFERENCES 
[Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936).
:::

# :question: Motivations

## Why Single-Neuron Modelling? {.smaller}

![](images/articles/2012Naud.png){.absolute top=78 left=0 width=48%}
![](images/articles/2013Druckmann.png){.absolute bottom=80 left=0 width=48%}
![](images/articles/2011Rossant.png){.absolute top=78 right=0 width=40%}
![](images/articles/2011Naud.png){.absolute bottom=0 right=0 width=45%}

::: {.aside}
<br> <br> <br> <br> 
Figures from [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208), [Rossant et al. (2011)](https://doi.org/10.3389/fnins.2011.00009), <br> [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en) and [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28).
:::

::: {.notes}
WHY NEURONAL MODELLING? / WHY THIS PROJECT? / WHY A SINGLE-NEURON MODELLING FRAMEWORK?  

FOUNDATIONS OF NEUROSCIENCE

- Unraveling and characterising the dynamics of the single neuron, the basic biophysical unit of neural information processing, is one of the most fundamental problems in theoretical, computational and systems neuroscience.
- Single-neuron models often have a large number of free parameters that must be constrained / optimised / estimated. Despite the seemingly straightforward nature of this problem, the main challenge is properly defining and quantifying the match between a given model and a target data set (the similarity / error / distance / loss / cost function).
- Over the last 20 years many researchers have shown that it is not only the temporally averaged firing rate that carries information about the stimulus, but also the exact timing of spikes. Therefore, if spike timing is important, a whole series of questions arises: What is the precision of spike timing if the same stimulus is repeated several times? Do spikes always appear at the same time? What would be a sensible measure of spike timing precision and reliability? Can a neuron model match the spike timing precision of a real neuron? Does it matter which neuron or what stimulus we take? If so, what would be a useful stimulus? More generally: is it possible to predict the spike times of a neuron with millisecond precision?

INTERNATIONAL COMPETITIONS [DESCRIBE WITH MUCH MORE DETAILS AND ADD TO SLIDES!]

- The above challenge has been turned into a single-neuron modeling competition that was first run by Brain Mind Institute at the Ecole Polytechnique Fe ́de ́rale de Lausanne (EPFL) in Switzerland and was officially handed over to the International Neuroinformatic Coordinating Facility (INCF) in Sweden in 2009.
- The idea behind the competition was that a good model can predict neuronal activity based on data that were not adopted in the parameter training / fitting phase. 
- The first spike timing competition in 2007 used an Ornstein-Uhlenbeck current injection with various means and variance to mimic the combined effect of a large number of synapses. In 2008, the competition was modified to replace the dynamic current by dynamic inhibitory and excitatory conductances using dynamic clamp. Then in 2009 the injected current was changed to a current produced by the simulation of six populations of presynaptic neurons changing their firing rate every 200-500 ms.
- Conclusions: High prediction performance is possible in many neuronal systems and **depends strongly on the choice of model structure and calibration method**. One important model feature for high prediction performance is the presence of spike-frequency adaptation. The choice of the model formalism can also influence the fitting method that can be used. High quality prediction is most of the time associated with an efficient and convex fitting method.
- "The data of the challenge will remain available in the future for bench-marking purposes, leaving the possibility for such a deed to be accomplished".
- Among the lessons to be learned from the INCF competition is that every neu- ron is different and one should not think of “the” model of a pyramidal cell or interneuron. Rather, parameters need to be tuned on a neuron-by-neuron basis. Another lesson is that the quality of a neuron model has to be measured on new data that are not accessible during the phase of parameter tuning.

METHODOLOGICAL DEVELOPMENTS 

Many recently (and not so recently) developed calibration / optimisation methods (Bayesian, evolutionary, mono- and multi-objective, etc.) are dramatically under-explored in the theoretical and computational neuroscience literature.

LIMITED PROGRESS IN 10+ YEARS

New optimisation / calibration algorithms have been developed ([AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)) which improved fitting when compared with the old ones between 5 and 8% (coincidence factor).

REFERENCES 

- [Jolivet et al. (2008)](https://doi.org/10.1016/j.jneumeth.2007.11.006) 
- [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x)
- [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936)
- [Rossant et al. (2011)](https://doi.org/10.3389/fnins.2011.00009)
- [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208)
- [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en)
- [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28)
- [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615)
- [Mohacsi et al. (2020)](https://doi.org/10.1109/ijcnn48605.2020.9206692)
- [AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)
:::

## Why Julia Language? {.smaller}

![](images/articles/2023Roesch-1.webp){.absolute bottom=80 left=0 width=50%}
![](images/articles/2023Roesch-2.webp){.absolute bottom=120 right=-200 width=65%}

::: {.aside}
Figures from [Roesch et al. (2023)](https://doi.org/10.1038/s41592-023-01832-z).
:::

::: {.notes}
WHY JULIA? 

- Julia solves the two-language problem: users do not have to choose between ease of use and high performance. 
- Julia is user friendly. It is easy to code.
- Julia is a high performance language. It is fast.
- Julia offers a high level of abstraction. It is flexible.
- Julia can be used for metaprogramming. It can code automatically.
- Julia is not only good in one area but in many. It enables “one language” projects.
- Easy to learn due to intuitive semantics and easy to read syntax.
- Accessible via various interfaces, REPL, IDE, or Jupyter notebook.
- Existing non-Julia code can be easily integrated into new Julia projects via language specific packages (interoperability).
- Julia is free, open-source and hosted on GitHub.
- Julia offers (generally) excellent documentation, tutorials, and help available directly from active and welcoming community members via various communication channels such as Slack, Discourse, Twitter or Zulip.
- Julia’s package ecosystem provides functionality for a wide range of oft-performed tasks in computational biology research.
- Julia code is smoothly extendable which enables and encourages easy contributions and collaborations to/with existing projects, as well as writing, integrating and sharing new, user specific packages.

REFERENCES

[Roesch et al. (2023)](https://doi.org/10.1038/s41592-023-01832-z).
:::

---

*Julia writes like Python but executes as fast as C...*

::: {.fragment}
*...assuming you know how to use it.*
:::

---

### Performance: Functions versus Methods
In programming, usually each function has several methods, one per combination of its arguments' types: 

```{.julia}
methods(+)[1:3] # 206 total methods at the time of writing
```
```{.nothing}
[1] +(x::T, y::T) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} in Base at int.jl:87
[2] +(x::T, y::T) where T<:Union{Float16, Float32, Float64} in Base at float.jl:383
[3] +(c::Union{UInt16, UInt32, UInt8}, x::BigInt) in Base.GMP at gmp.jl:531     
```

---


### Performance: Method selection

Whenever a function is called with certain arguments, the right method has to be selected. The method selection has two parts:

1. (Arguments') Type Inference;
2. Method lookup.

---

### Performance: Type inference at compile time

```{.julia}
julia> @code_warntype(MultilayerGraphs.add_node!(multilayerdigraph, Node("new_node"); add_vertex_to_layers = Symbol[]))
MethodInstance for (::MultilayerGraphs.var"#add_node!##kw")(::NamedTuple{(:add_vertex_to_layers,), Tuple{Vector{Symbol}}}, ::typeof(MultilayerGraphs.add_node!), ::MultilayerDiGraph{Int64, Float64}, ::Node)
  from (::MultilayerGraphs.var"#add_node!##kw")(::Any, ::typeof(MultilayerGraphs.add_node!), mg::MultilayerDiGraph, n::Node) in MultilayerGraphs at e:\other_drives\developer07\package_development\MultilayerGraphs\dev\MultilayerGraphs\src\multilayerdigraph.jl:221
Arguments
  _::Core.Const(MultilayerGraphs.var"#add_node!##kw"())
  @_2::NamedTuple{(:add_vertex_to_layers,), Tuple{Vector{Symbol}}}
  @_3::Core.Const(MultilayerGraphs.add_node!)
  mg::MultilayerDiGraph{Int64, Float64}
  n::Node
Locals
  add_vertex_to_layers::Vector{Symbol}
  @_7::Vector{Symbol}
Body::Bool
1 ─ %1  = Base.haskey(@_2, :add_vertex_to_layers)::Core.Const(true)
│         Core.typeassert(%1, Core.Bool)
│   %3  = Base.getindex(@_2, :add_vertex_to_layers)::Vector{Symbol}
│   %4  = Core.apply_type(MultilayerGraphs.Vector, MultilayerGraphs.Symbol)::Core.Const(Vector{Symbol})
│   %5  = Core.apply_type(MultilayerGraphs.Union, %4, MultilayerGraphs.Symbol)::Core.Const(Union{Symbol, Vector{Symbol}})
│   %6  = (%3 isa %5)::Core.Const(true)
│         Core.typeassert(%6, Core.Bool)
└──       goto #3
2 ─       Core.Const(:(Core.apply_type(MultilayerGraphs.Vector, MultilayerGraphs.Symbol)))
│         Core.Const(:(Core.apply_type(MultilayerGraphs.Union, %9, MultilayerGraphs.Symbol)))
│         Core.Const(:(%new(Core.TypeError, Symbol("keyword argument"), :add_vertex_to_layers, %10, %3)))
└──       Core.Const(:(Core.throw(%11)))
3 ┄       (@_7 = %3)
└──       goto #5
4 ─       Core.Const(:(@_7 = Base.getindex(MultilayerGraphs.Symbol)))
5 ┄ %16 = @_7::Vector{Symbol}
│         (add_vertex_to_layers = %16)
│   %18 = (:add_vertex_to_layers,)::Core.Const((:add_vertex_to_layers,))
│   %19 = Core.apply_type(Core.NamedTuple, %18)::Core.Const(NamedTuple{(:add_vertex_to_layers,)})
│   %20 = Base.structdiff(@_2, %19)::Core.Const(NamedTuple())
│   %21 = Base.pairs(%20)::Core.Const(Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}())
│   %22 = Base.isempty(%21)::Core.Const(true)
│         Core.typeassert(%22, Core.Bool)
└──       goto #7
6 ─       Core.Const(:(Base.kwerr(@_2, @_3, mg, n)))
7 ┄ %26 = MultilayerGraphs.:(var"#add_node!#592")(add_vertex_to_layers, @_3, mg, n)::Bool
└──       return %26
```

---

### Performance: Summary on Performance

*If successful, type inference at compile time will also trigger method lookup at compile time, which has the potential to speed up code by orders of magnitude.*


# [Practice / Implementation]{.semi-transparent}

[In this section, we are going to see:]{.semi-transparent}

1. Why Julia?

    [1. Performance;]{.semi-transparent}

    2. Metaprogramming;

    [3. Easy-to-work-with environment.]{.semi-transparent}

[2. Overview of Graphs.jl;]{.semi-transparent}

[3. MultilayerGraphs.jl's main design features;]{.semi-transparent}

[4. MultilayerGraphs.jl's future developments;]{.semi-transparent}

[5. Brief showcase/tutorial.]{.semi-transparent}

---

## Why Julia?: Metaprogramming

It refers to the set of features that allows a program to generate and execute parts of its code at run time. An example from the Julia Manual:

```{.julia}
for op = (:sin, :cos, :tan, :log, :exp)
    eval(quote
        Base.$op(a::MyNumber) = MyNumber($op(a.x))
    end)
end
```
Notable applications are Domain Specific Languages (DSLs) such as Turing.jl.


# :bookmark_tabs: Methods 

1.  Models 
2.  Calibration

::: {.notes}
Computational neuroscientists use mathematical models built on observational data to investigate what’s happening in the brain. Models can simulate brain activity from the behavior of a single neuron right through to the patterns of collective activity in whole neural networks. Collecting the experimental data is the first step, then the challenge becomes deciding which computer models best represent the data and can explain the underlying causes of how the brain behaves.

Researchers usually find the right model for their data through trial and error. This involves tweaking a model’s parameters until the model can reproduce the data of interest. But this process is laborious and not systematic. Moreover, with the ever-increasing complexity of both data and computer models in neuroscience, the old-school approach of building models is starting to show its limitations.
:::

# [Methods]{.semi-transparent} {.smaller}

1.  Models:

    2.1 Leaky Integrate and Fire

    2.2 Hodgkin-Huxley

    2.3 Two Dimensional Reduction

    2.4 One Dimensional Reduction

    2.5 Exponential NLIF

    2.6 AdEx-NLIF

2.  [Calibration]{.semi-transparent}

## Models: Leaky integrate and fire {.smaller}

- Subthreshold dynamics:
$$
\tau \frac{du}{dt} = -(u(t) - u_{rest}) + RI(T)
$${#eq-lif}

![](images/books/2014Gerstner-LIF_circuit.png){.absolute top=90 left=850 width=25%}

- With firing mechanism: 

$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$${#eq-lif_reset}

. . .

- Cannot describe Bursting, Adaptation, Delayed Response, Inhibitory Rebound etc.

. . .


- Cannot describe spike features such as spike height, width at half height, etc...

::: {.notes}
Comes from considering the neuron as the circuit in fig A.
:::

## Models: Hodgkin-Huxley {.smaller}

$$
C\frac{du}{dt} = -\sum\limits_k I_k(t) + I(t)
$${#eq-hh_u}

. . .


Where, in the simplest case ^[[Gerstner et al. (2014)](https://neuronaldynamics.epfl.ch/online/Ch2.S2.html)]:

$$
\sum\limits_k I_k(t) = g_{Na}m^3h(u - E_{Na}) + g_Kn^4(u-E_K) + g_L(u-E_L)
$${#eq-hh_currents}

. . .

Where, setting $x \in \{m,n,h\}$:

$$
\dot{x} = \alpha_x(u)(1-x) - \beta_x(u)x
$${#eq-gating_variable_exp_diffeq}

. . .

Where, for instance:

$$
\alpha_m(u) = \frac{0.182(u+35)}{1-e^\frac{u+35}{9}} \; , \; \beta_m(u)= \frac{-0.124(u+35)}{1-e^\frac{u+35}{9}}
$${#eq-boltzmann_terms_exp}

## Models: Hodgkin-Huxley {.smaller}

**Unstable** $\rightarrow$ define^[[Toth et al. (2011)](https://doi.org/10.1007/s00422-011-0459-1)]:

$$
x_0(u) := \frac{\alpha_x(u)}{\alpha_x(u) +  \beta_x(u)} = \frac{1}{2}\left(1+\tanh\left(\frac{u - u_x}{\kappa_x}\right)\right)
$${#eq-x_0}
$$
\tau_x(u) = \frac{1}{\alpha_x(u) +  \beta_x(u)} = \tau^0_x + \tau^{max}_x\left(1-\tanh\left( \frac{u - u_{\tau,x}}{\sigma_x} \right)^2\right)
$${#eq-tau}

. . .

And then substitute @eq-gating_variable_exp_diffeq with:

$$
\dot{x} = \frac{x_0(u) - x(u)}{\tau_x(u)}
$${#eq-gating_variables_tanh_diffeq}

. . .

$\rightarrow$ solver stability.

::: {.notes}
The exponential form is numerically unstable → we use the $\tanh$ formalism 1. We define:

. . .

Parameters values must then be fitted to the shapes of @eq-boltzmann_terms_exp

. . .

Which achieves solver stability.
:::

## Models: two dimensional reduction {.smaller}

Hodgkin-Huxley models are a bit **unpractical**:

- No voltage nor current thresholds:

::: {layout-nrow=1}
![](images/books/2014Gerstner-no_voltage_threshold.png){width=500 height=300}
![](images/books/2014Gerstner-no_current_threshold.png){width=500 height=300}
:::

. . .

- Impossible to visualize and analyze a 4d+ dynamical system;

. . .

- Very hard to identify parameters.

::: {.notes}
- Usually required for intuitiveness
- 
- The dynamical role of parameters is often obscure

To these, we'd also add a certain increase in the nuance required to implement and calibrate HH systems w.r.t. NLIF that we're about to see.
:::

## Models: two dimensional reduction {.smaller}

Note: dynamics of $n$ and $h$ are very slow w.r.t. $m$

![](images/books/2014Gerstner-tau_x){.absolute top=90 left=650 width=40%}

. . .


::: {.absolute top=250}
$\rightarrow$ define $:w = b-h = an$


::: {.fragment}
So that @eq-hh_u becomes: 
$$
\frac{du}{dt} = \frac{1}{\tau}(F(u,w) + RI(t))
$$
:::

::: {.fragment}
And @eq-gating_variable_exp_diffeq (or @eq-gating_variables_tanh_diffeq) reduce to: 
$$
\frac{dw}{dt} = \frac{1}{\tau_w}G(u,w)
$$
:::

::: {.fragment}
Examples: Morris-Lecar model, FitzHugh-Nagumo model
:::
:::


## Models: one dimensional reduction {.smaller}

Define $\epsilon := \frac{\tau}{\tau_w}$. If $\tau \gt \gt \tau_w$:

![](images/books/2014Gerstner-one_dimensional_reduction_firing_mechanism)<!-- {.absolute top=90 left=650 width=40%} -->

. . .

::: {.absolute top=80 left=430}
Since:

- One may **NOT** interested in spike shape;

::: {.fragment}
- The $w$ variable only varies **after** the voltage has reached its maximum.
:::
:::

. . .

$\rightarrow$ substitute the equation for $\frac{dw}{dt}$ with reset mechanism! $\rightarrow$ **NLIF** models:

$$
\frac{du}{dt} = \frac{1}{\tau}(F(u,w_{rest}) + RI(t)) =: \frac{1}{\tau}(f(u) + RI(t))
$${#eq-NLIF} 
$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$${#eq-one_dimensional_reduction}



::: {.notes}
Excitability can now be discussed with the help of Fig. 4.21. A current pulse shifts the state of the system horizontally away from the stable fixed point. If the current pulse is small, the system returns immediately (i.e., on the fast time scale) to the stable fixed point. If the current pulse is large enough so as to put the system beyond the middle branch of the u-nullcline, then the trajectory is pushed toward the right branch of the u-nullcline. The trajectory follows the u-nullcline slowly upward until it jumps back (on the fast time scale) to the left branch of the u-nullcline. The ‘jump’ between the branches of the nullcline corresponds to a rapid voltage change. In terms of neuronal modeling, the jump from the right to the left branch corresponds to the downstroke of the action potential. The middle branch of the u-nullcline (where u˙>0) acts as a threshold for spike initiation;

. . .
- they're roughly all the smae -> they carry no information
-
. . .

w_rest is the w at the intersection of the nullclines. Below is the firing mechanism. $\delta$ can be interpreted as the "dead time"
:::

## Models: Exponential NLIF {.smaller}

How to choose $f(u)$?

. . .

![](images/books/2014Gerstner-AdEx_derivation){height=200}

::: {.absolute top=150 left=550}
$$
\frac{f(u(t))}{\tau} = \left<\frac{I(t)}{C} - \frac{du}{dt}\right>
$$
:::

. . .

$\rightarrow$ best fitting function is: $f(u)= -(u-u_{rest}) + \Delta_T e^{\frac{u-\theta_{rh}}{\Delta_T}}$

. . .

<!-- Would get the same result by approximating the $F(u,w_{rest})$ of a 2-dimensional Hodgkin-Huxley model: -->

![](images/books/2014Gerstner-AdEx_derivation_hh){height=200}

::: {.absolute top=500 left=550}
Same result from reduced 1d Hodgkin Huxley model.
:::

::: {.notes}
By performing experiments:

In order to determine the function f~(u), an experimentalist injects a time-dependent current I(t) into the soma of a neuron while measuring with a second electrode the voltage u(t). From the voltage time course, one finds the voltage derivative du/dt.

A measurement at time t yields a value u(t) (which we use as value along the x-axis of a plot) and a value [(I(t)/C)−(du/dt)] (which we plot along the y axis). With a thousand or more time points per second, the plot fills up rapidly. For each voltage u there are many data points with different values along the y-axis. The best choice of the parameter C is the one that minimizes the width of this distribution. At the end, we average across all points at a given voltage u to find the empirical function (35)

. . .

We find that the empirical function extracted from experiments is well approximated by a combination of a linear and exponential term

. . .

In figure you see a plot of the $F(u, {w_rest})$ of a reduced to 2d HH model where we make the further assumption to make it 1d (separation of time scales). There a 2 fixed points, the leftmost is stable (u_rest), the one in the middle is unstable and denotes the effective threshold. The rightmost one ends the upswing of the spike. One could discard the rightmost and introduce a numerical threshold $\theta_{reset}$ together with a reset mechanism. This way, what's left of the $F(u, {w_rest})$ can be approximated with the $f(u)$ of the AdEx above.

:::


## Models: AdEx-NLIF {.smaller}

A single @eq-NLIF cannot capture all firing patterns $\rightarrow$ add abstract **linear** current:

$$
\tau\frac{du}{dt} =f(u) -R\sum\limits_k w_k + RI(t) \; , \; \{\tau_k\frac{dw_k}{dt} = a_k(u-u_{rest}) -w_k +b_k\tau_k\sum\limits_{t^{(f)}}\delta(t-t^{(f)})\}
$$
$$
u(t) \ge \theta \implies u(t+\delta) = u_r
$$

. . .

![](images/books/2014Gerstner-firing_patterns){height=300 width=800}

::: {.notes}
The δ-function in the wk equations indicates that, during firing, the adaptation currents wk are increased by an amount bk.The parameters bk  are the ‘jump’ of the spike-triggered adaptation.

. . .

Spike-triggered adaptation is controlled by a combination of a and b. The choice of a and b largely determines the firing patterns of the neuron. In figure, you may see al firing patterns as a function of a and b. FOr every pattern, an hight and a low step current are reported.
:::

# [Methods]{.semi-transparent} {.smaller}

1.  [Model Specification]{.semi-transparent}
2.  Model Calibration:

    3.1 Intro

    3.2 Features

    3.3 Feature selection
    
    3.4 MOO

    3.5 BORG-MOEA

    3.6 Pareto-optimal front

## Calibration: Intro

-   **Model calibration** find a parameter set $\vec{w}$ that makes the model *match* experimental trace;

. . .

-   The notion of "matching" will be made precise by the definition of **objective functions** (**loss**);

. . .

-   Calibration methods will need to take into account the **intrinsic stochasticity** of neurons.

:::{.notes}
- "Model calibration" refers to the task of finding a parameter set $\vec{w}$ that makes the model *match* experimental trace;
- i.e. functions that measure the degree to which the model is capable of reproducing experimental traces along certain dimensions;
- i.e. their tendency to produce different voltage traces when injected with the same input.
:::

## Calibration: Features {.smaller}

We consider two classes of objective functions:

. . .

1.  **Trace-to-trace (direct) comparisons**:

    -   L2 loss;
    -   Phase plane trajectory distance [^1];
    -   ...

[^1]: [LeMasson and Maex (2001)](https://www.taylorfrancis.com/chapters/edit/10.1201/9781420039290-8/introduction-equation-solving-parameter-fitting)

. . .

2.  **Feature-based distance functions** [^2]:

    -   Firing frequency;
    -   Spike height;
    -   ...

[^2]: [Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)

::: notes
1.  These are losses that compare traces point-wise
2.  These don't take point-wise differences
:::

## Calibration: Features {.smaller}

*Trace-to-trace (direct) comparisons* usually [^3] perform poorly, since:

[^3]: [We haven't yet considered more sophisticated approach like Brookings et al. (2014)](https://doi.org/10.1152/jn.00007.2014)

-   Sub- and supra-threshold dynamics are *unbalanced*;

. . .

-   Small *shifts* between simulated and measured spikes usually yield large *L2* errors.

. . .


-   *Intrinsic stochasticity* makes the selection of the experimental repetition $r \in R$ to match *arbitrary*;

::: notes
-   

-   in the $V$ and in the $(V, \frac{dV}{dt})$ spaces;

-   
:::

## Calibration: Features {.smaller}

*Feature-based distance functions* avoid these issues, because:


. . .

-   They are not point-wise, thus insensitive to *regime imbalances*;

. . .

-   They directly make the model reproduce *qualitative* features of the target neuron;

. . .

-   They take into account *Intrinsic stochasticity*:

. . .

By defining the loss relative to feature $f$ w.r.t. repetition set $R$ :  $$L_f(\vec{w}, R) = \frac{|\bar{f}_R - f(s(\vec{w}))|}{\sigma^f_R}$$

<!-- $L_f(\vec{w}, R)$   *Intrinsic stochasticity* within repetition set $R$ w.r.t. feature $f$ can be dealt with by defining the loss: -->

. . .

- We say that we have a good fit when every objective is under 2 SD ^[[Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)].

::: notes
-   

-   

Where:

-   $R$ is a set of voltage traces produced by the same neuron injected with the same current
-   $\vec{w}$ is a set of model parameters
-   $s(\vec{w})$ is a voltage trace simulated by the model with parameters $\vec{w}$
-   $\bar{f}_R$ is the average value of feature $f$ over $R$
-   $\sigma^f_R$ is the standard deviation of $f$ over $R$
-   $f(s(\vec{w}))$ is the value of feature $f$ over $s(\vec{w})$
:::


## Calibration: Feature selection {.smaller .scrollable}

Experiments ^[[Druckmann et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002133)] suggest the following associations:

. . .

- Square inputs:

  - Firing frequency;
  - Latency;
  - Accomodation index;
  - Width of AP at half height (only for HH models);
  - Average height of AP;
  - Slow throughs;

. . .

- Ramp inputs:

  - Average height of AP;
  - Slow throughs;
  - Slope of the fit of the spike peaks;

. . .

- Colored noisy inputs:

  - Coincidence factor;
  - Control-adjusted RMS ^[[Brookings et al. (2014)](https://doi.org/10.1152/jn.00007.2014)].



## Calibration: MOO

> How to combine the different features?

. . .

**Weighted sum**? No:

-   Choice of weights is not obvious;

. . .

-   No guarantee on the nature and quality of the compromise;

. . .

$\rightarrow$ **Multi-Objective Optimization** !


## Calibration: MOO {.smaller}

**Def.** Given the objectives $\{L_{f_1}, L_{f_2}, ..., L_{f_n}\}$, a set of repetitions $R$ and two parameter sets $\vec{w_1}$ and $\vec{w_2}$ we say that $\vec{w_1}$ **dominates** $\vec{w_2}$ $\iff$:

1.  $L_{f_i}(\vec{w_1},R) \geq L_{f_i}(\vec{w_2},R) \qquad \forall i= 1,...,n$
2.  $\exists \bar{i} \in \{1,..,n\} | L_{f_\bar{i}}(\vec{w_1},R) \gt L_{f_\bar{i}}(\vec{w_2},R)$

. . .

MOO purpose: find the set of all $\vec{w}$ that do not dominate each other.

. . .

Such set is the **Pareto-optimal front** (**POF**).


## Calibration: BORG-MOEA

- Calibration algorithm: [BORG-MOEA](http://borgmoea.org/);

. . .

- It is an evolutionary algorithm with self-tuning capabilities;

. . .

- Pitting it against other algorithms determined its advantage.

## Calibration: Pareto-optimal front {.smaller}

- The purpose of a *MOEA* is to approximate the *POF* by finding some of its elements;

. . .

- Projection of the *POF* set may assume two configurations:

![Projections of the Pareto optimal front along two sets of features ^[[Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)]](images/articles/2007Druckmann_POF.png){height=200 width=800}

. . .

- First configuration $\rightarrow$ the two objectives do not conflict each other;

. . .

- Second configuration $\rightarrow$ the two objectives do conflict each other;


## Calibration: Pareto-optimal front 

- Calibration algorithms work better the less objectives they have;

. . .

- $\rightarrow$ one may think to sum together features repeated across stimuli;

. . .

- Only possible if the two features are **NOT** in a tradeoff ^[[Druckmann (2013)](https://doi.org/10.1007/s00422-008-0269-2)]


:::{.notes}
-
-
- In fact if they are not in a tradeoff the calibration algorithm, can minimize the sum of the two. Instead if they are in a tradeoff, if we sum them the algorithm will find a compromise whose nature and quality we're not guaranteed of 8we'd be bak to the problem in the MOO #1 slide.
:::



## Calibration: Pareto-optimal front #3

- Configuration of the projections of the POF are not known a priori $\rightarrow$ features must be kept separate;

. . .

- To anyway reduce the number of features, one may resort to objectives combinations ^[[Hay et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002107)];

. . .

- They consists in substituting two features with the maximum or other combinations of the two. 

::: notes
:::

# :computer: Tools 

::: {.notes}
:::

# :package: Related Packages 

## Related Packages 

:::: {.columns}

::: {.column width="50%"}
### Julia

- [**BlackBoxOptim**](https://github.com/robertfeldt/BlackBoxOptim.jl)
- [**DifferentialEquations**](https://github.com/SciML/DifferentialEquations.jl)
- [**Evolutionary**](https://github.com/wildart/Evolutionary.jl)
- [**Metaheuristics**](https://github.com/jmejia8/Metaheuristics.jl)
- [**ModelingToolkit**](https://github.com/SciML/ModelingToolkit.jl)
- [**Optim**](https://github.com/JuliaNLSolvers/Optim.jl)
- [Turing](https://github.com/TuringLang/Turing.jl)
- [Conductor](https://github.com/wsphillips/Conductor.jl)

![](images/logo/JuliaDots.svg){.absolute top=83 left=130 width=5%}
:::

::: {.column width="50%"}
### Python 

- [**AllenSDK**](https://github.com/AllenInstitute/AllenSDK)
- [BluePyEfe](https://github.com/BlueBrain/BluePyEfe)
- [BluePyOpt](https://github.com/BlueBrain/BluePyOpt)
- [**eFEL**](https://github.com/BlueBrain/eFEL)
- [NEURON](https://github.com/neuronsimulator/nrn)
- [Optimizer](https://github.com/KaliLab/optimizer)
- [PyNEST](https://nest-simulator.readthedocs.io/en/stable/tutorials/index.html)

![](images/logo/Python.png){.absolute top=83 right=260 width=5%}
:::

::::

# :books: References

## References {.smaller}

:::: {.columns}

::: {.column width="35%"}
### Present 

- [Brette and Gerstner (2005)](https://doi.org/10.1152/jn.00686.2005)
- [Pospischil et al. (2008)](https://doi.org/10.1007/s00422-008-0263-8)
- [Druckmann et al. (2007)](https://doi.org/10.3389/neuro.01.1.1.001.2007)
- [Druckmann et al. (2008)](https://doi.org/10.1007/s00422-008-0269-2) 
- [Jolivet et al. (2008)](https://doi.org/10.1007/s00422-008-0261-x)
- [Gerstner and Naud (2009)](https://doi.org/10.1126/science.1181936)
- [Druckmann et al. (2011)](https://doi.org/10.1371/journal.pcbi.1002133) 
- [Toth et al. (2011)](https://doi.org/10.1007/s00422-011-0459-1)
- [Naud et al. (2012)](https://infoscience.epfl.ch/record/168813?ln=en)
- [Druckmann (2013)](https://doi.org/10.1007/978-1-4614-8094-5_28)
- [Gerstner et al. (2014)](https://doi.org/10.1017/cbo9781107447615)
:::

::: {.column width="35%"}

### Future 

- [Van Geit et al. (2008)](https://doi.org/10.1007/s00422-008-0257-6) 
- [Kobayashi et al. (2009)](https://doi.org/10.3389/neuro.10.009.2009)
- [Smolinski et al. (2009)](https://doi.org/10.1186/1471-2202-10-s1-p260)
- [Naud et al. (2011)](https://doi.org/10.1162/neco_a_00208)
- [Svensson et al. (2012)](https://doi.org/10.1007/s12021-012-9140-7)
- [Friedrich et al. (2014)](https://doi.org/10.3389/fninf.2014.00063)
- [Druckmann (2014)](https://doi.org/10.1007/978-1-4614-7320-6_159-1)
- [Meliza et al. (2014)](https://doi.org/10.1007/s00422-014-0615-5) 
- [Lynch and Houghton (2015)](https://doi.org/10.3389/fninf.2015.00010)
- [Van Geit et al. (2016)](https://doi.org/10.3389/fninf.2016.00017)
- [Masoli et al. (2017)](https://doi.org/10.3389/fncel.2017.00071)
:::

::: {.column width="30%"}

### Future 

- [Gonçalves et al. (2020)](https://doi.org/10.7554/elife.56261)
- [Mohacsi et al. (2020)](https://doi.org/10.1109/ijcnn48605.2020.9206692)
- [Saray et al. (2021)](https://doi.org/10.1371/journal.pcbi.1008114)
- [AbdelAty et al. (2022)](https://doi.org/10.3389/fninf.2022.771730)
- [Kreuz et al. (2022)](https://doi.org/10.1007/978-1-0716-1006-0_409)
- [Nandi et al. (2022)](https://doi.org/10.1016/j.celrep.2022.111176)
:::
::::

::: {.notes}
...
:::

# Thanks :pray:

---

## {.smaller}

### Outline 

:one: [Objectives](#Objectives)

:two: [Motivations](#Motivations)

:three: [Methods](#Methods)

:four: [Tools](#Tools)

:five: [Results](#Results)

### Resources 

![](images/logo/JuliaDots.svg){.absolute bottom=252 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [ModellingFramework.jl](https://github.com/InPhyT/ModellingFramework.jl) 

![](images/logo/JuliaDots.svg){.absolute bottom=205 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Package: [NeuronalModelling.jl](https://github.com/InPhyT/NeuronalModelling.jl) 

![](images/logo/GitHub.svg){.absolute bottom=155 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2007)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2007) 

![](images/logo/GitHub.svg){.absolute bottom=107 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Competition (2009)](https://github.com/InPhyT/Quantitative_Single_Neuron_Modeling_Competition_2009) 

![](images/logo/GitHub.svg){.absolute bottom=59 left=0 width=3.2%} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data: [Allen Brain Atlas (2023)](https://github.com/AllenInstitute/AllenSDK) 

![](images/logo/NeuronalModelling.svg){.absolute bottom=120 right=0 width=45%}

::: {.notes}
...
:::